<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Multilingual NLP / Multilinguisme en Traitement des langues - Machine Translation, Language Models</title><link href="https://fyvo.github.io/" rel="alternate"></link><link href="https://fyvo.github.io/feeds/machine-translation-language-models.atom.xml" rel="self"></link><id>https://fyvo.github.io/</id><updated>2020-10-09T00:00:00+02:00</updated><subtitle>Un site académique / An academic site</subtitle><entry><title>Very Very Large Transformers</title><link href="https://fyvo.github.io/posts/2020/oct/09/very-very-large-transformers.html" rel="alternate"></link><published>2020-10-09T00:00:00+02:00</published><updated>2020-10-09T00:00:00+02:00</updated><author><name>François Yvon</name></author><id>tag:fyvo.github.io,2020-10-09:/posts/2020/oct/09/very-very-large-transformers.html</id><summary type="html">&lt;h2&gt;Transformers Models: Basics&lt;/h2&gt;
&lt;p&gt;The transformer architecture of &lt;a href=""&gt;(Vaswani et al, 2017)&lt;/a&gt; has been transformative in the sense that is has quickly replaced more classical RNN architectures as the basic block for building large-scale structured prediction model. A TF learns to transform a structured linguistic object (typically a string) into a …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Transformers Models: Basics&lt;/h2&gt;
&lt;p&gt;The transformer architecture of &lt;a href=""&gt;(Vaswani et al, 2017)&lt;/a&gt; has been transformative in the sense that is has quickly replaced more classical RNN architectures as the basic block for building large-scale structured prediction model. A TF learns to transform a structured linguistic object (typically a string) into a sequence of numerical representations that can then be used for multiple purposes: machine translation, as in the original paper; language modeling and more generally text generation (summarisation, simplification, etc), and more recently representation learning via the &lt;strong&gt;masked language modelling&lt;/strong&gt; task &lt;a href=""&gt;(Devlin et al, 2019)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In words, a TF learns to turn context-free lexical embedding into contextual representations: each computation layer recombines in a series of 2 main operations all the input representations, to yield the input for the next layer. Given a sentence represented as a list of vectors &lt;span class="math"&gt;\(I^l= [h_1^{l-1} \dots h_T^{l-1}]\)&lt;/span&gt; at the input of layer &lt;span class="math"&gt;\(l\)&lt;/span&gt;, the most basic operation is the &lt;strong&gt;attention operation&lt;/strong&gt; which will compute a similarity between each input &lt;span class="math"&gt;\(h_i^{l-1}\)&lt;/span&gt; and all the other inputs in &lt;span class="math"&gt;\(I_l\)&lt;/span&gt; in a vector &lt;span class="math"&gt;\(\alpha^l\)&lt;/span&gt; of length &lt;span class="math"&gt;\(T\)&lt;/span&gt;. Once normalized in &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;, these coefficients are used to compute the updated representation &lt;span class="math"&gt;\(h_i^{l} = \sum_j \alpha_i^l h_j^{l-1}\)&lt;/span&gt;, which will be then be added to &lt;span class="math"&gt;\(h_i\)&lt;/span&gt;, normalized, and passed through a feed forward layer before being propagated to the upper layers (details omitted).&lt;/p&gt;
&lt;p&gt;Since the first publication, these models have been extensively dissected and analyzed, a very nice starting point beeing the &lt;a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"&gt;literate reimplementation by S. Rush&lt;/a&gt; or the &lt;a href="https://jalammar.github.io/illustrated-transformer/"&gt;illustrated version by J. Alammar&lt;/a&gt;. Open source implementations abound, and trained models for multiple tasks and language can be downloaded from the &lt;a href="https://huggingface.co/transformers/"&gt;HugginFace Transformer repos&lt;/a&gt;. Many derived models have been proposed extending the capacities of Transformers in several directions giving rise to a &lt;a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html"&gt;full family of models&lt;/a&gt;. in the sequel, we focus on attempts at developing larger models, also recently surveyed in &lt;a href=""&gt;(Tay et al, 2020)&lt;/a&gt;. &lt;/p&gt;
&lt;h2&gt;Extending the depth&lt;/h2&gt;
&lt;p&gt;GPT-* denotes a family of models of increasing size and depth introduced over the years by scholars from &lt;a href="www.openai.com"&gt;OpenAi&lt;/a&gt;, (which incidentally, is &lt;a href="https://techcrunch.com/2019/03/11/openai-shifts-from-nonprofit-to-capped-profit-to-attract-capital"&gt;less and less open&lt;/a&gt;). The core architecture is a &lt;a href=""&gt;Transformer language model&lt;/a&gt; with self-attention, trained with an increasing number of heads and layers. GPT-2 was already so powerful that the large-scale models could not be released to the public (sic!); &lt;a href="https://arxiv.org/pdf/2005.14165.pdf"&gt;GTP-3&lt;/a&gt; is a much larger beast (a 100x increase with respect to GPT-2), comprising 175B parameters, and trained with a corpus of more than 400B (BPE) tokens collected from multiple sources that are represented in the &lt;a href="https://commoncrawl.org/"&gt;Common Crawl corpus&lt;/a&gt; where a large part of the effort has been cleaning the corpus.&lt;/p&gt;
&lt;p&gt;Typical parameter size of the model are in the &lt;a href="(https://arxiv.org/pdf/2005.14165.pdf)"&gt;cited paper&lt;/a&gt; Table 2.1, where we see variations accross the number of heads (from 12 to 96), their size (64 or 128), the number of layers (from 12 to 96), the internal dimension of the vectors (from 768 = 12&lt;em&gt;64 to 12288 = 96&lt;/em&gt;128) and the size batch. The length of context is fixed (2048 BPE tokens). Accomodating such numbers also implies a remarkable engineering effort (distribution etc), and small adaptations to the original transformer model to accomodate sparsity in the attention computation. The authors just mention using 'alternating dense and locally banded sparseattention patterns in the layers of the transformer, similar to the &lt;a href=""&gt;Sparse Transformer&lt;/a&gt;', plus access to "a high-bandwidth cluster provided by Microsoft" (most likely containing a very large number of GPus V100 cards) - possibly related to &lt;a href="https://venturebeat.com/2020/05/19/openai-microsoft-azure-supercomputer-ai-model-training/"&gt;MS anouncement of an AI-based computing infrastructure&lt;/a&gt; hosting 10000 V100 GPU cards.&lt;/p&gt;
&lt;h3&gt;GPT-3 and understanding&lt;/h3&gt;
&lt;p&gt;https://medium.com/@ChrisGPotts/is-it-possible-for-language-models-to-achieve-language-understanding-81df45082ee2&lt;/p&gt;
&lt;h3&gt;Artificial tests&lt;/h3&gt;
&lt;p&gt;GPT-3 was also tested with symbolic calculus tasks on numbers (additions, multiplications, etc) and strings (anagrams, read backwards). Only the largest models with zero-shot / few-shot learning does anything good; in particular it solves 2 and 3 digit addition / substraction almost perfectly. Multiplication and string problems seem much more difficult. The issues of GPT-3 with string problems has been analyzed in depth by &lt;a href="https://medium.com/@melaniemitchell.me/can-gpt-3-make-analogies-16436605c446"&gt;Melanie Mitchell&lt;/a&gt;, who knows some' about &lt;a href="https://melaniemitchell.me/PapersContent/HofstadterMitchellCopycat1995.pdf"&gt;string problems&lt;/a&gt;. Her conclusions are worth reproducing here: "All in all, GPT-3’s performance is often impressive and surprising, but it is also similar to a lot of what we see in today’s state-of-the-art AI systems: impressive, intelligent-seeming performance interspersed with unhumanlike errors, plus no transparency as to why it performs well or makes certain errors. And it is often hard to tell if the system has actually learned the concept we are trying to teach it."&lt;/p&gt;
&lt;h3&gt;Multilinguality in GPT-3&lt;/h3&gt;
&lt;p&gt;For English the number of words: 181 014 683 608 - 181M (meaning BPE would more than double this ?), for French 3 553 061 536 (3.5M) which is only 1.81% still already monstruous - &lt;a href="https://camembert-model.fr/"&gt;CamemBERT&lt;/a&gt;  contains 23B, &lt;a href="https://arxiv.org/pdf/1912.05372.pdf"&gt;Flaubert&lt;/a&gt;  only 12B. Another view on this number is that it would correspond to about All these numbers are from the OpenAI's &lt;a href="https://github.com/openai/gpt-3/tree/master/dataset_statistics"&gt;github repos&lt;/a&gt;. Numbers for translation are reasonably bad (when fully unsupervised); with few shot supervision it gets much better especially when translating into English.&lt;/p&gt;
&lt;p&gt;The exact details of the task are in the appendix. For the zero-shot model, the prompt is "What is the French Translation of "This English Sentence ?". For the few shot models, the training examples were was "This  English sentence = Cette phrase française"&lt;/p&gt;
&lt;p&gt;Additional sources:
* https://medium.com/@bechr7/gpt-3-an-overview-da5d503c9ea8
* https://www.gwern.net/GPT-3#prompts-as-programming&lt;/p&gt;
&lt;h2&gt;Enlarging the context&lt;/h2&gt;
&lt;p&gt;Another direction of research has been the extension of the context window. While the initial model only considered a sentencial context, the need to enlarge the context window has quickly emerged, for instance to make the output of a language generating transformer more consistent, or, in machine translation, to model supra sentential phenomena, or more generally discourse related phenomena.&lt;/p&gt;
&lt;p&gt;Recall that in a nutshell, the Transformer architecture transforms a structured context (a sequence of tokens, could also be a tree or a graph) into a single numerical vector. The core operation in this transformation is the iterative computation of each individual token's representation based on their similarity to other tokens in the context. In equations, representation &lt;span class="math"&gt;\(\tilde{h}_i^{kl}\)&lt;/span&gt; for position &lt;span class="math"&gt;\(i\)&lt;/span&gt; in layer &lt;span class="math"&gt;\(l\)&lt;/span&gt; is computed by head &lt;span class="math"&gt;\(k\)&lt;/span&gt; as:&lt;/p&gt;
&lt;div class="math"&gt;$$\tilde{h}_i^{kl} =  \operatorname{softmax} (\frac{1}{\sqrt{d_k}} h_i^{l-1} Q^{kl} [H^{l} K^{kl}]^T) H^{l} V^{kl}$$&lt;/div&gt;
&lt;p&gt;with &lt;span class="math"&gt;\(Q^{kl}, K^{kl}, V^{kl}\)&lt;/span&gt; parameter matrices associated to these head and layer, &lt;span class="math"&gt;\(d\)&lt;/span&gt; is the model internal dimension, and &lt;span class="math"&gt;\(d_k\)&lt;/span&gt; is the head size with value &lt;span class="math"&gt;\(d/k\)&lt;/span&gt;. The output of these &lt;span class="math"&gt;\(k\)&lt;/span&gt; computations are  then concatenated and passed through a feedforward layer with ReLU activation; each of these steps also includes a summation with &lt;span class="math"&gt;\(H^{l-1}\)&lt;/span&gt; and a layer normalization (see Figure, borrowed from &lt;a href="https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa"&gt;this  post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Head computation from" src="https://fyvo.github.io/images/transformer-head-computation.png"&gt;&lt;/p&gt;
&lt;p&gt;These operations are performed simultaneously for all positions (and for a full batch of them), meaning that the matrix operations actually imply the whole of &lt;span class="math"&gt;\(H^l\)&lt;/span&gt; instead of just one line or one column. The attention computation has complexity &lt;span class="math"&gt;\(O(T^2)\)&lt;/span&gt;, is performed &lt;span class="math"&gt;\(k\)&lt;/span&gt; times for each of the &lt;span class="math"&gt;\(l\)&lt;/span&gt; layers; for gradient computation, it is also needed to store the entire attention matrix of size &lt;span class="math"&gt;\((T^2)\)&lt;/span&gt;. These are the main obstacles towards enlarging the transformer context.&lt;/p&gt;
&lt;h3&gt;Passing information between contexts&lt;/h3&gt;
&lt;p&gt;A remarkable benefit of the Transformer architecture is that it makes consecutive contexts independent from each other, enabling to parallelize the computation. &lt;a href="https://www.aclweb.org/anthology/P19-1285.pdf"&gt;(Dai et al, 2019)&lt;/a&gt; explain why this can also be viewed as an issue, both computationally (the same computations are repeated multiple times) and from a model perspective (the system has a fixed past horizon). Their proposal, &lt;strong&gt;Transformer-XL&lt;/strong&gt; is to reintroduce some recurrence, and to augment the context representation with hidden states computed at past time frames (note that these values are fixed, and do not propagate any gradients to earlier time frames). &lt;/p&gt;
&lt;h3&gt;Improving the time complexity&lt;/h3&gt;
&lt;p&gt;Improving on the time complexity requires to speed up the dot product operations involved in the attention. There are of course generic methods to speed up computation (eg. use half precision float representations), that we do not discuss any further here. One generic method that does not work though, is to use sparse matrix operations, which are hard to accelerate on GPUs. [004.07320]&lt;/p&gt;
&lt;p&gt;One specific way to proceed is to reduce the number of neighbours to a fixed size. In &lt;a href="https://openreview.net/forum?id=Hyg0vbWC-"&gt;(Liu et al, 2018)&lt;/a&gt; this is first done by restricting the attention computation to blocks of limited size: this means that the representation of a token only recombines the representation of tokens within the same block. This reduces the contextualization of token representations, but also creates boundary effects at the frontier between block. An alternative (&lt;strong&gt;&lt;em&gt;Memory compressed attention&lt;/em&gt;&lt;/strong&gt;) explored in the same work use strided convolutions to reduce the number of neighbors, which preserving the global context.&lt;/p&gt;
&lt;p&gt;Boundary effects can also be avoided by considering neighbors in a (sliding) windows of &lt;span class="math"&gt;\(S\)&lt;/span&gt; words, which means that only the near-diagonal terms of the attention matrix will be computed. If context is localized in the lower layers, it still remains global at the upper layers as the influence of more distant words diffuses in the network. A further trick is to "dilate" these local contexts to speed up the diffusion in the network. To preserve the overall performance, a critical aspect is to make sure that a restricted number of positions still keep a global view over the full input, meaning that they attend to (and are attended to) by all positions. These positions can be described as performing local summaries that are propagated through the entire network. Having one such position every &lt;span class="math"&gt;\(\sqrt{T}\)&lt;/span&gt; block of length &lt;span class="math"&gt;\(\sqrt{T}\)&lt;/span&gt; ensures an overall &lt;span class="math"&gt;\(O(T\sqrt{T})\)&lt;/span&gt; complexity for the attention computation.
Such methods are used notably in the &lt;strong&gt;Sparse Transformer&lt;/strong&gt;  of &lt;a href="https://arxiv.org/pdf/1904.10509"&gt;(Child et al, 2019)&lt;/a&gt;, in the &lt;strong&gt;Longformer&lt;/strong&gt; of &lt;a href="http://arxiv.org/pdf/2004.05150"&gt;Beltagy et al (2020)&lt;/a&gt;, and also employed in the GPT-3 architecture &lt;a href="https://arxiv.org/pdf/2005.14165.pdf"&gt;(Brown et al 2020)&lt;/a&gt; presented above. Introduced in &lt;a href="https://arxiv.org/pdf/1911.04070.pdf"&gt;(Ye et al, 2020)&lt;/a&gt;, the &lt;strong&gt;Binary-Tree Transformer&lt;/strong&gt; describes an alternative way to combine local and global attention patterns by explicitly organizing the propagation of attention in a (binary) tree structure: each token's representation recombines local neighbors, as well as distant tokens whose representations are first condensed in &lt;strong&gt;&lt;em&gt;spans&lt;/em&gt;&lt;/strong&gt; representations organized in a tree, which means that each token only needs to compute attention scores with &lt;span class="math"&gt;\(O(\log(T))\)&lt;/span&gt; other nodes. In this approach, the root span representation remains the only representation that integrates the full context in its representation. On a related issue, &lt;a href="https://www.aclweb.org/anthology/2020.acl-main.672/"&gt;(Rae \&amp;amp; Razavi, 2020)&lt;/a&gt; show that global attention is more important in the upper than in the lower levels.&lt;/p&gt;
&lt;p&gt;The paper by &lt;a href="http://arxiv.org/2007.14062"&gt;Zaheer et al, (2020)&lt;/a&gt; finally introduces the &lt;strong&gt;Long Bird&lt;/strong&gt; which somehow generalizes these ideas by introducing &lt;strong&gt;&lt;em&gt;&lt;a href="https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa"&gt;neighbor graphs&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt; specifying, for each position &lt;span class="math"&gt;\(i\)&lt;/span&gt;, the sets of other positions that are used in the computation of &lt;span class="math"&gt;\(\tilde{h}^{kl}_i\)&lt;/span&gt;. In addition to the local context, and to &lt;strong&gt;&lt;em&gt;global tokens&lt;/em&gt;&lt;/strong&gt;, the authors propose to also add a random component by adding random neighbors - by choosing specific random graphs, they show that these random neighbors help speed up "diffusion" of information amongst tokens. Moreover, these authors also prove that it is possible to enforce sparsity in the neighbor graph without changing the computational ability of the architecture (which remains Turing complete). Also note that these approaches will imply the definition of supplementary meta-parameters: size of the context window, number (and position) of "global" tokens, size of the random graph, etc.&lt;/p&gt;
&lt;p&gt;Note that parsity can also be enforced without setting a parameter, by replacing the costly &lt;span class="math"&gt;\(\operatorname{softmax}\)&lt;/span&gt; operator by the no-less costly &lt;span class="math"&gt;\(\operatorname{sparsemax}\)&lt;/span&gt; &lt;a href="http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf"&gt;(Niculae &amp;amp; Blondel, 2017)&lt;/a&gt;, which however will make sure that &lt;span class="math"&gt;\(\alpha^{kl}\)&lt;/span&gt; mostly contains &lt;span class="math"&gt;\(0\)&lt;/span&gt;, which could speed up further computations. This approach is described in &lt;a href="https://www.aclweb.org/anthology/D19-1223.pdf"&gt;(Correia et al, 2019)&lt;/a&gt;, where the focus is however more on the interpretability side than on computational issues. Indeed, learned sparsity patterns make the contribution of each head much easier to interprete.&lt;/p&gt;
&lt;p&gt;Another way to speed up this computation is to use approximations: in the &lt;strong&gt;Reformer&lt;/strong&gt; of &lt;a href="http://arxiv.org/pdf/2001.0445"&gt;Kitaev et al, (2019)&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;locally-sensitive hashing&lt;/em&gt;&lt;/strong&gt; is used to identifiy the most significant terms in the summation (corresponding to the most similar neighbours), thereby also yielding sparse attention matrices and computational gains. Another, much simpler way, to save compute time (and space) is to take &lt;span class="math"&gt;\(Q^{kl} = K^{lk}\)&lt;/span&gt;, which seems to have hardly any impact on performance. The &lt;strong&gt;Linformer&lt;/strong&gt; approach of &lt;a href="https://arxiv.org/pdf/2006.04768.pdf"&gt;Wang et al (2020)&lt;/a&gt; rests on the observation that the computation performed by heads can be approximated by the product of two low ranks matrices.
Furthermore, these low rank matrices can be obtained by introducing two random matrices for each head, one to project the &lt;span class="math"&gt;\(V^{kl} I^{l-1}\)&lt;/span&gt; term (&lt;span class="math"&gt;\(T \times{} d_k)\)&lt;/span&gt; into a &lt;span class="math"&gt;\((S \times{} d_k)\)&lt;/span&gt; matrix (through the multiplication by a &lt;span class="math"&gt;\(S \times T\)&lt;/span&gt; matrix), the other to project &lt;span class="math"&gt;\(K^{kl} I^{l-1}\)&lt;/span&gt; also into a &lt;span class="math"&gt;\((S \times d_k )\)&lt;/span&gt; matrix. As a result, the term in the &lt;span class="math"&gt;\(\operatorname{softmax}\)&lt;/span&gt; outputs a &lt;span class="math"&gt;\(T\times S\)&lt;/span&gt; matrix (instead of &lt;span class="math"&gt;\(T \times t\)&lt;/span&gt;). By choosing &lt;span class="math"&gt;\(T \gg S\)&lt;/span&gt;, we get the announced complexity reduction, at almost zero cost in terms of performance. As in other papers, the authors show that parameter sharing (here sharing the projection matrices across layers) can also help speed up the computation without harming performance (too much).&lt;/p&gt;
&lt;p&gt;The Performer: http://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html&lt;/p&gt;
&lt;p&gt;The "Sinkhorn transformer" &lt;a href="https://arxiv.org/abs/2002.11296v1"&gt;Sparse Sinkhorn attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Linear Transformer&lt;/p&gt;
&lt;p&gt;The Fastformer [https://arxiv.org/abs/2108.09084]&lt;/p&gt;
&lt;h3&gt;[Improving the space complexity]&lt;/h3&gt;
&lt;p&gt;Use revertible gradients - (Kitaev et al)&lt;/p&gt;
&lt;p&gt;use gradient checkpointing (Chen et al., 2016) saves memoryby only caching activations of a subset of layers. This does save space, at the cost of an increased training time.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Translation, Language Models"></category></entry></feed>