<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Multilingual NLP / Multilinguisme en Traitement des langues</title><link href="https://fyvo.github.io/" rel="alternate"></link><link href="https://fyvo.github.io/feeds/all.atom.xml" rel="self"></link><id>https://fyvo.github.io/</id><updated>2021-01-01T00:00:00+01:00</updated><subtitle>Un site académique / An academic site</subtitle><entry><title>Reading Group - GDR TAL</title><link href="https://fyvo.github.io/posts/2021/jan/01/reading-group-gdr-tal-en.html" rel="alternate"></link><published>2021-01-01T00:00:00+01:00</published><updated>2021-01-01T00:00:00+01:00</updated><author><name>François Yvon</name></author><id>tag:fyvo.github.io,2021-01-01:/posts/2021/jan/01/reading-group-gdr-tal-en.html</id><summary type="html">&lt;h2&gt;October 30, 2020&lt;/h2&gt;
&lt;h3&gt;&lt;a href="https://arxiv.org/pdf/2002.02955.pdf"&gt;A multilingual view of unsupervised MT&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Garcia, Foret, Sellam and Parikh&lt;/p&gt;
&lt;p&gt;Devise a model for multilingual unsupevised MT. The main idea is as follows: consider &lt;span class="math"&gt;\(L\)&lt;/span&gt; languages, and model the joint distribution &lt;span class="math"&gt;\(P(x,y,z...)\)&lt;/span&gt; (let us assume &lt;span class="math"&gt;\(L=3\)&lt;/span&gt; for the sake of the argument …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;October 30, 2020&lt;/h2&gt;
&lt;h3&gt;&lt;a href="https://arxiv.org/pdf/2002.02955.pdf"&gt;A multilingual view of unsupervised MT&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Garcia, Foret, Sellam and Parikh&lt;/p&gt;
&lt;p&gt;Devise a model for multilingual unsupevised MT. The main idea is as follows: consider &lt;span class="math"&gt;\(L\)&lt;/span&gt; languages, and model the joint distribution &lt;span class="math"&gt;\(P(x,y,z...)\)&lt;/span&gt; (let us assume &lt;span class="math"&gt;\(L=3\)&lt;/span&gt; for the sake of the argument) based on a multiplicity of monolingual or bilingual corpora. The translation parameters require conditional models so the main objective is a sum
&lt;/p&gt;
&lt;div class="math"&gt;$$
    \log E_{y,z} P_{\theta}(x|y,z) + \log E_{x,z}  P_{\theta}(y|x,z)  + \log E_{x,y} P_{\theta}(z|x,y) 
$$&lt;/div&gt;
&lt;p&gt;
where the unobserved source data (source) are handled as latent variables in the model. A major assumption is that we do not need &lt;span class="math"&gt;\(y,z\)&lt;/span&gt; to generate the translation &lt;span class="math"&gt;\(x\)&lt;/span&gt;, hence
$ P_{\theta}(x|y,z)= P_{\theta}(x|y)= P_{\theta}(x|z) = \sqrt{P_{\theta}(x|y)P_{\theta}(x|z)}.&lt;/p&gt;
&lt;p&gt;Each term in the summation is lower bounded using the Jensen's inequality, yielding for instance for the first term:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\log E_{y,z} P_{\theta}(x|y,z) \ge \frac{1}{2} E_{y \sim P_\theta(y|x)} \logP(x|y) + \frac{1}{2} E_{y \sim P_\theta(y|z)} \logP(x|z) + E_{(y,z) \sim Y,Z} \log P(y,z)
$$&lt;/div&gt;
&lt;p&gt;
Il est intéressant de voir les deux premiers termes comme des termes de reconstruction après back-translation.
It is interesting to see the first two terms as reconstruction after a back-translation.&lt;/p&gt;
&lt;p&gt;As all these terms are expectations, one can try to use EM to maximise this bound; during the E step one must compute the posterior of &lt;span class="math"&gt;\(y|x\)&lt;/span&gt;, which is approximated using the sole &lt;span class="math"&gt;\(argmax\)&lt;/span&gt; (itself an approximation) &lt;span class="math"&gt;\(\widehat{y}\)&lt;/span&gt;; during the &lt;span class="math"&gt;\(M-step\)&lt;/span&gt; one must optimize in &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, but instead the authors propose to perform one step of gradient update. &lt;/p&gt;
&lt;h3&gt;&lt;a href="https://arxiv.org/pdf/2006.04768.pdf"&gt;The Linformer&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Linformer&lt;/strong&gt; approach of &lt;a href="https://arxiv.org/pdf/2006.04768.pdf"&gt;Wang et al (2020)&lt;/a&gt; rests on the observation that the computation performed by heads can be approximated by the product of two low ranks matrices.&lt;/p&gt;
&lt;p&gt;Furthermore, these low rank matrices can be obtained by introducing two random matrices for each head, one to project the &lt;span class="math"&gt;\(V^{kl} I^{l-1}\)&lt;/span&gt; term (&lt;span class="math"&gt;\(T \times{} d_k)\)&lt;/span&gt; into a &lt;span class="math"&gt;\((S \times{} d_k)\)&lt;/span&gt; matrix (through the multiplication by a &lt;span class="math"&gt;\(S \times T\)&lt;/span&gt; matrix), the other to project &lt;span class="math"&gt;\(K^{kl} I^{l-1}\)&lt;/span&gt; also into a &lt;span class="math"&gt;\((S \times d_k )\)&lt;/span&gt; matrix. As a result, the term in the &lt;span class="math"&gt;\(\operatorname{softmax}\)&lt;/span&gt; outputs a &lt;span class="math"&gt;\(T\times S\)&lt;/span&gt; matrix (instead of &lt;span class="math"&gt;\(T \times t\)&lt;/span&gt;). By choosing &lt;span class="math"&gt;\(T \gg S\)&lt;/span&gt;, we get the announced complexity reduction, at almost zero cost in terms of performance. As in other papers, the authors show that parameter sharing (here sharing the projection matrices across layers) can also help speed up the computation without harming performance (too much).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category></entry><entry><title>Very Very Large Transformers</title><link href="https://fyvo.github.io/posts/2020/oct/09/very-very-large-transformers.html" rel="alternate"></link><published>2020-10-09T00:00:00+02:00</published><updated>2020-10-09T00:00:00+02:00</updated><author><name>François Yvon</name></author><id>tag:fyvo.github.io,2020-10-09:/posts/2020/oct/09/very-very-large-transformers.html</id><summary type="html">&lt;h2&gt;Transformers Models: Basics&lt;/h2&gt;
&lt;p&gt;The transformer architecture of &lt;a href=""&gt;(Vaswani et al, 2017)&lt;/a&gt; has been transformative in the sense that is has quickly replaced more classical RNN architectures as the basic block for building large-scale structured prediction model. A TF learns to transform a structured linguistic object (typically a string) into a …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Transformers Models: Basics&lt;/h2&gt;
&lt;p&gt;The transformer architecture of &lt;a href=""&gt;(Vaswani et al, 2017)&lt;/a&gt; has been transformative in the sense that is has quickly replaced more classical RNN architectures as the basic block for building large-scale structured prediction model. A TF learns to transform a structured linguistic object (typically a string) into a sequence of numerical representations that can then be used for multiple purposes: machine translation, as in the original paper; language modeling and more generally text generation (summarisation, simplification, etc), and more recently representation learning via the &lt;strong&gt;masked language modelling&lt;/strong&gt; task &lt;a href=""&gt;(Devlin et al, 2019)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In words, a TF learns to turn context-free lexical embedding into contextual representations: each computation layer recombines in a series of 2 main operations all the input representations, to yield the input for the next layer. Given a sentence represented as a list of vectors &lt;span class="math"&gt;\(I^l= [h_1^{l-1} \dots h_T^{l-1}]\)&lt;/span&gt; at the input of layer &lt;span class="math"&gt;\(l\)&lt;/span&gt;, the most basic operation is the &lt;strong&gt;attention operation&lt;/strong&gt; which will compute a similarity between each input &lt;span class="math"&gt;\(h_i^{l-1}\)&lt;/span&gt; and all the other inputs in &lt;span class="math"&gt;\(I_l\)&lt;/span&gt; in a vector &lt;span class="math"&gt;\(\alpha^l\)&lt;/span&gt; of length &lt;span class="math"&gt;\(T\)&lt;/span&gt;. Once normalized in &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;, these coefficients are used to compute the updated representation &lt;span class="math"&gt;\(h_i^{l} = \sum_j \alpha_i^l h_j^{l-1}\)&lt;/span&gt;, which will be then be added to &lt;span class="math"&gt;\(h_i\)&lt;/span&gt;, normalized, and passed through a feed forward layer before being propagated to the upper layers (details omitted).&lt;/p&gt;
&lt;p&gt;Since the first publication, these models have been extensively dissected and analyzed, a very nice starting point beeing the &lt;a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"&gt;literate reimplementation by S. Rush&lt;/a&gt; or the &lt;a href="https://jalammar.github.io/illustrated-transformer/"&gt;illustrated version by J. Alammar&lt;/a&gt;. Open source implementations abound, and trained models for multiple tasks and language can be downloaded from the &lt;a href="https://huggingface.co/transformers/"&gt;HugginFace Transformer repos&lt;/a&gt;. Many derived models have been proposed extending the capacities of Transformers in several directions giving rise to a &lt;a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html"&gt;full family of models&lt;/a&gt;. in the sequel, we focus on attempts at developing larger models, also recently surveyed in &lt;a href=""&gt;(Tay et al, 2020)&lt;/a&gt;. &lt;/p&gt;
&lt;h2&gt;Extending the depth&lt;/h2&gt;
&lt;p&gt;GPT-* denotes a family of models of increasing size and depth introduced over the years by scholars from &lt;a href="www.openai.com"&gt;OpenAi&lt;/a&gt;, (which incidentally, is &lt;a href="https://techcrunch.com/2019/03/11/openai-shifts-from-nonprofit-to-capped-profit-to-attract-capital"&gt;less and less open&lt;/a&gt;). The core architecture is a &lt;a href=""&gt;Transformer language model&lt;/a&gt; with self-attention, trained with an increasing number of heads and layers. GPT-2 was already so powerful that the large-scale models could not be released to the public (sic!); &lt;a href="https://arxiv.org/pdf/2005.14165.pdf"&gt;GTP-3&lt;/a&gt; is a much larger beast (a 100x increase with respect to GPT-2), comprising 175B parameters, and trained with a corpus of more than 400B (BPE) tokens collected from multiple sources that are represented in the &lt;a href="https://commoncrawl.org/"&gt;Common Crawl corpus&lt;/a&gt; where a large part of the effort has been cleaning the corpus.&lt;/p&gt;
&lt;p&gt;Typical parameter size of the model are in the &lt;a href="(https://arxiv.org/pdf/2005.14165.pdf)"&gt;cited paper&lt;/a&gt; Table 2.1, where we see variations accross the number of heads (from 12 to 96), their size (64 or 128), the number of layers (from 12 to 96), the internal dimension of the vectors (from 768 = 12&lt;em&gt;64 to 12288 = 96&lt;/em&gt;128) and the size batch. The length of context is fixed (2048 BPE tokens). Accomodating such numbers also implies a remarkable engineering effort (distribution etc), and small adaptations to the original transformer model to accomodate sparsity in the attention computation. The authors just mention using 'alternating dense and locally banded sparseattention patterns in the layers of the transformer, similar to the &lt;a href=""&gt;Sparse Transformer&lt;/a&gt;', plus access to "a high-bandwidth cluster provided by Microsoft" (most likely containing a very large number of GPus V100 cards) - possibly related to &lt;a href="https://venturebeat.com/2020/05/19/openai-microsoft-azure-supercomputer-ai-model-training/"&gt;MS anouncement of an AI-based computing infrastructure&lt;/a&gt; hosting 10000 V100 GPU cards.&lt;/p&gt;
&lt;h3&gt;GPT-3 and understanding&lt;/h3&gt;
&lt;p&gt;https://medium.com/@ChrisGPotts/is-it-possible-for-language-models-to-achieve-language-understanding-81df45082ee2&lt;/p&gt;
&lt;h3&gt;Artificial tests&lt;/h3&gt;
&lt;p&gt;GPT-3 was also tested with symbolic calculus tasks on numbers (additions, multiplications, etc) and strings (anagrams, read backwards). Only the largest models with zero-shot / few-shot learning does anything good; in particular it solves 2 and 3 digit addition / substraction almost perfectly. Multiplication and string problems seem much more difficult. The issues of GPT-3 with string problems has been analyzed in depth by &lt;a href="https://medium.com/@melaniemitchell.me/can-gpt-3-make-analogies-16436605c446"&gt;Melanie Mitchell&lt;/a&gt;, who knows some' about &lt;a href="https://melaniemitchell.me/PapersContent/HofstadterMitchellCopycat1995.pdf"&gt;string problems&lt;/a&gt;. Her conclusions are worth reproducing here: "All in all, GPT-3’s performance is often impressive and surprising, but it is also similar to a lot of what we see in today’s state-of-the-art AI systems: impressive, intelligent-seeming performance interspersed with unhumanlike errors, plus no transparency as to why it performs well or makes certain errors. And it is often hard to tell if the system has actually learned the concept we are trying to teach it."&lt;/p&gt;
&lt;h3&gt;Multilinguality in GPT-3&lt;/h3&gt;
&lt;p&gt;For English the number of words: 181 014 683 608 - 181M (meaning BPE would more than double this ?), for French 3 553 061 536 (3.5M) which is only 1.81% still already monstruous - &lt;a href="https://camembert-model.fr/"&gt;CamemBERT&lt;/a&gt;  contains 23B, &lt;a href="https://arxiv.org/pdf/1912.05372.pdf"&gt;Flaubert&lt;/a&gt;  only 12B. Another view on this number is that it would correspond to about All these numbers are from the OpenAI's &lt;a href="https://github.com/openai/gpt-3/tree/master/dataset_statistics"&gt;github repos&lt;/a&gt;. Numbers for translation are reasonably bad (when fully unsupervised); with few shot supervision it gets much better especially when translating into English.&lt;/p&gt;
&lt;p&gt;The exact details of the task are in the appendix. For the zero-shot model, the prompt is "What is the French Translation of "This English Sentence ?". For the few shot models, the training examples were was "This  English sentence = Cette phrase française"&lt;/p&gt;
&lt;p&gt;Additional sources:
* https://medium.com/@bechr7/gpt-3-an-overview-da5d503c9ea8
* https://www.gwern.net/GPT-3#prompts-as-programming&lt;/p&gt;
&lt;h2&gt;Enlarging the context&lt;/h2&gt;
&lt;p&gt;Another direction of research has been the extension of the context window. While the initial model only considered a sentencial context, the need to enlarge the context window has quickly emerged, for instance to make the output of a language generating transformer more consistent, or, in machine translation, to model supra sentential phenomena, or more generally discourse related phenomena.&lt;/p&gt;
&lt;p&gt;Recall that in a nutshell, the Transformer architecture transforms a structured context (a sequence of tokens, could also be a tree or a graph) into a single numerical vector. The core operation in this transformation is the iterative computation of each individual token's representation based on their similarity to other tokens in the context. In equations, representation &lt;span class="math"&gt;\(\tilde{h}_i^{kl}\)&lt;/span&gt; for position &lt;span class="math"&gt;\(i\)&lt;/span&gt; in layer &lt;span class="math"&gt;\(l\)&lt;/span&gt; is computed by head &lt;span class="math"&gt;\(k\)&lt;/span&gt; as:&lt;/p&gt;
&lt;div class="math"&gt;$$\tilde{h}_i^{kl} =  \operatorname{softmax} (\frac{1}{\sqrt{d_k}} h_i^{l-1} Q^{kl} [H^{l} K^{kl}]^T) H^{l} V^{kl}$$&lt;/div&gt;
&lt;p&gt;with &lt;span class="math"&gt;\(Q^{kl}, K^{kl}, V^{kl}\)&lt;/span&gt; parameter matrices associated to these head and layer, &lt;span class="math"&gt;\(d\)&lt;/span&gt; is the model internal dimension, and &lt;span class="math"&gt;\(d_k\)&lt;/span&gt; is the head size with value &lt;span class="math"&gt;\(d/k\)&lt;/span&gt;. The output of these &lt;span class="math"&gt;\(k\)&lt;/span&gt; computations are  then concatenated and passed through a feedforward layer with ReLU activation; each of these steps also includes a summation with &lt;span class="math"&gt;\(H^{l-1}\)&lt;/span&gt; and a layer normalization (see Figure, borrowed from &lt;a href="https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa"&gt;this  post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Head computation from" src="https://fyvo.github.io/images/transformer-head-computation.png"&gt;&lt;/p&gt;
&lt;p&gt;These operations are performed simultaneously for all positions (and for a full batch of them), meaning that the matrix operations actually imply the whole of &lt;span class="math"&gt;\(H^l\)&lt;/span&gt; instead of just one line or one column. The attention computation has complexity &lt;span class="math"&gt;\(O(T^2)\)&lt;/span&gt;, is performed &lt;span class="math"&gt;\(k\)&lt;/span&gt; times for each of the &lt;span class="math"&gt;\(l\)&lt;/span&gt; layers; for gradient computation, it is also needed to store the entire attention matrix of size &lt;span class="math"&gt;\((T^2)\)&lt;/span&gt;. These are the main obstacles towards enlarging the transformer context.&lt;/p&gt;
&lt;h3&gt;Passing information between contexts&lt;/h3&gt;
&lt;p&gt;A remarkable benefit of the Transformer architecture is that it makes consecutive contexts independent from each other, enabling to parallelize the computation. &lt;a href="https://www.aclweb.org/anthology/P19-1285.pdf"&gt;(Dai et al, 2019)&lt;/a&gt; explain why this can also be viewed as an issue, both computationally (the same computations are repeated multiple times) and from a model perspective (the system has a fixed past horizon). Their proposal, &lt;strong&gt;Transformer-XL&lt;/strong&gt; is to reintroduce some recurrence, and to augment the context representation with hidden states computed at past time frames (note that these values are fixed, and do not propagate any gradients to earlier time frames). &lt;/p&gt;
&lt;h3&gt;Improving the time complexity&lt;/h3&gt;
&lt;p&gt;Improving on the time complexity requires to speed up the dot product operations involved in the attention. There are of course generic methods to speed up computation (eg. use half precision float representations), that we do not discuss any further here. One generic method that does not work though, is to use sparse matrix operations, which are hard to accelerate on GPUs. [004.07320]&lt;/p&gt;
&lt;p&gt;One specific way to proceed is to reduce the number of neighbours to a fixed size. In &lt;a href="https://openreview.net/forum?id=Hyg0vbWC-"&gt;(Liu et al, 2018)&lt;/a&gt; this is first done by restricting the attention computation to blocks of limited size: this means that the representation of a token only recombines the representation of tokens within the same block. This reduces the contextualization of token representations, but also creates boundary effects at the frontier between block. An alternative (&lt;strong&gt;&lt;em&gt;Memory compressed attention&lt;/em&gt;&lt;/strong&gt;) explored in the same work use strided convolutions to reduce the number of neighbors, which preserving the global context.&lt;/p&gt;
&lt;p&gt;Boundary effects can also be avoided by considering neighbors in a (sliding) windows of &lt;span class="math"&gt;\(S\)&lt;/span&gt; words, which means that only the near-diagonal terms of the attention matrix will be computed. If context is localized in the lower layers, it still remains global at the upper layers as the influence of more distant words diffuses in the network. A further trick is to "dilate" these local contexts to speed up the diffusion in the network. To preserve the overall performance, a critical aspect is to make sure that a restricted number of positions still keep a global view over the full input, meaning that they attend to (and are attended to) by all positions. These positions can be described as performing local summaries that are propagated through the entire network. Having one such position every &lt;span class="math"&gt;\(\sqrt{T}\)&lt;/span&gt; block of length &lt;span class="math"&gt;\(\sqrt{T}\)&lt;/span&gt; ensures an overall &lt;span class="math"&gt;\(O(T\sqrt{T})\)&lt;/span&gt; complexity for the attention computation.
Such methods are used notably in the &lt;strong&gt;Sparse Transformer&lt;/strong&gt;  of &lt;a href="https://arxiv.org/pdf/1904.10509"&gt;(Child et al, 2019)&lt;/a&gt;, in the &lt;strong&gt;Longformer&lt;/strong&gt; of &lt;a href="http://arxiv.org/pdf/2004.05150"&gt;Beltagy et al (2020)&lt;/a&gt;, and also employed in the GPT-3 architecture &lt;a href="https://arxiv.org/pdf/2005.14165.pdf"&gt;(Brown et al 2020)&lt;/a&gt; presented above. Introduced in &lt;a href="https://arxiv.org/pdf/1911.04070.pdf"&gt;(Ye et al, 2020)&lt;/a&gt;, the &lt;strong&gt;Binary-Tree Transformer&lt;/strong&gt; describes an alternative way to combine local and global attention patterns by explicitly organizing the propagation of attention in a (binary) tree structure: each token's representation recombines local neighbors, as well as distant tokens whose representations are first condensed in &lt;strong&gt;&lt;em&gt;spans&lt;/em&gt;&lt;/strong&gt; representations organized in a tree, which means that each token only needs to compute attention scores with &lt;span class="math"&gt;\(O(\log(T))\)&lt;/span&gt; other nodes. In this approach, the root span representation remains the only representation that integrates the full context in its representation. On a related issue, &lt;a href="https://www.aclweb.org/anthology/2020.acl-main.672/"&gt;(Rae \&amp;amp; Razavi, 2020)&lt;/a&gt; show that global attention is more important in the upper than in the lower levels.&lt;/p&gt;
&lt;p&gt;The paper by &lt;a href="http://arxiv.org/2007.14062"&gt;Zaheer et al, (2020)&lt;/a&gt; finally introduces the &lt;strong&gt;Long Bird&lt;/strong&gt; which somehow generalizes these ideas by introducing &lt;strong&gt;&lt;em&gt;&lt;a href="https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa"&gt;neighbor graphs&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt; specifying, for each position &lt;span class="math"&gt;\(i\)&lt;/span&gt;, the sets of other positions that are used in the computation of &lt;span class="math"&gt;\(\tilde{h}^{kl}_i\)&lt;/span&gt;. In addition to the local context, and to &lt;strong&gt;&lt;em&gt;global tokens&lt;/em&gt;&lt;/strong&gt;, the authors propose to also add a random component by adding random neighbors - by choosing specific random graphs, they show that these random neighbors help speed up "diffusion" of information amongst tokens. Moreover, these authors also prove that it is possible to enforce sparsity in the neighbor graph without changing the computational ability of the architecture (which remains Turing complete). Also note that these approaches will imply the definition of supplementary meta-parameters: size of the context window, number (and position) of "global" tokens, size of the random graph, etc.&lt;/p&gt;
&lt;p&gt;Note that parsity can also be enforced without setting a parameter, by replacing the costly &lt;span class="math"&gt;\(\operatorname{softmax}\)&lt;/span&gt; operator by the no-less costly &lt;span class="math"&gt;\(\operatorname{sparsemax}\)&lt;/span&gt; &lt;a href="http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf"&gt;(Niculae &amp;amp; Blondel, 2017)&lt;/a&gt;, which however will make sure that &lt;span class="math"&gt;\(\alpha^{kl}\)&lt;/span&gt; mostly contains &lt;span class="math"&gt;\(0\)&lt;/span&gt;, which could speed up further computations. This approach is described in &lt;a href="https://www.aclweb.org/anthology/D19-1223.pdf"&gt;(Correia et al, 2019)&lt;/a&gt;, where the focus is however more on the interpretability side than on computational issues. Indeed, learned sparsity patterns make the contribution of each head much easier to interprete.&lt;/p&gt;
&lt;p&gt;Another way to speed up this computation is to use approximations: in the &lt;strong&gt;Reformer&lt;/strong&gt; of &lt;a href="http://arxiv.org/pdf/2001.0445"&gt;Kitaev et al, (2019)&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;locally-sensitive hashing&lt;/em&gt;&lt;/strong&gt; is used to identifiy the most significant terms in the summation (corresponding to the most similar neighbours), thereby also yielding sparse attention matrices and computational gains. Another, much simpler way, to save compute time (and space) is to take &lt;span class="math"&gt;\(Q^{kl} = K^{lk}\)&lt;/span&gt;, which seems to have hardly any impact on performance. The &lt;strong&gt;Linformer&lt;/strong&gt; approach of &lt;a href="https://arxiv.org/pdf/2006.04768.pdf"&gt;Wang et al (2020)&lt;/a&gt; rests on the observation that the computation performed by heads can be approximated by the product of two low ranks matrices.
Furthermore, these low rank matrices can be obtained by introducing two random matrices for each head, one to project the &lt;span class="math"&gt;\(V^{kl} I^{l-1}\)&lt;/span&gt; term (&lt;span class="math"&gt;\(T \times{} d_k)\)&lt;/span&gt; into a &lt;span class="math"&gt;\((S \times{} d_k)\)&lt;/span&gt; matrix (through the multiplication by a &lt;span class="math"&gt;\(S \times T\)&lt;/span&gt; matrix), the other to project &lt;span class="math"&gt;\(K^{kl} I^{l-1}\)&lt;/span&gt; also into a &lt;span class="math"&gt;\((S \times d_k )\)&lt;/span&gt; matrix. As a result, the term in the &lt;span class="math"&gt;\(\operatorname{softmax}\)&lt;/span&gt; outputs a &lt;span class="math"&gt;\(T\times S\)&lt;/span&gt; matrix (instead of &lt;span class="math"&gt;\(T \times t\)&lt;/span&gt;). By choosing &lt;span class="math"&gt;\(T \gg S\)&lt;/span&gt;, we get the announced complexity reduction, at almost zero cost in terms of performance. As in other papers, the authors show that parameter sharing (here sharing the projection matrices across layers) can also help speed up the computation without harming performance (too much).&lt;/p&gt;
&lt;p&gt;The Performer: http://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html&lt;/p&gt;
&lt;p&gt;The "Sinkhorn transformer" &lt;a href="https://arxiv.org/abs/2002.11296v1"&gt;Sparse Sinkhorn attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Linear Transformer&lt;/p&gt;
&lt;p&gt;The Fastformer [https://arxiv.org/abs/2108.09084]&lt;/p&gt;
&lt;h3&gt;[Improving the space complexity]&lt;/h3&gt;
&lt;p&gt;Use revertible gradients - (Kitaev et al)&lt;/p&gt;
&lt;p&gt;use gradient checkpointing (Chen et al., 2016) saves memoryby only caching activations of a subset of layers. This does save space, at the cost of an increased training time.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Translation, Language Models"></category></entry><entry><title>Notes on Optimal Transport for Multilingual Embeddings</title><link href="https://fyvo.github.io/posts/2020/oct/01/notes-on-optimal-transport-for-multilingual-embeddings.html" rel="alternate"></link><published>2020-10-01T00:00:00+02:00</published><updated>2020-10-01T00:00:00+02:00</updated><author><name>François Yvon</name></author><id>tag:fyvo.github.io,2020-10-01:/posts/2020/oct/01/notes-on-optimal-transport-for-multilingual-embeddings.html</id><summary type="html">&lt;p&gt;This post is a short summary of what needs to be known, and slightly more, to fully get the grasp of a series of not so recent papers (2017-2018) which have tried to exploit the mathematical theory of Optimal Transport to compute multilingual word embeddings.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Monge's Problem&lt;/h2&gt;
&lt;p&gt;Monge's problem is a mapping problem: given two sets of points with a mass &lt;span class="math"&gt;\(X= \{x_i, i=1 \dots n\}\)&lt;/span&gt; with respective mass &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y=\{y_j, j=1 \dots m\}\)&lt;/span&gt; with respective mass &lt;span class="math"&gt;\(b_j\)&lt;/span&gt; the &lt;em&gt;Monge assignment problem&lt;/em&gt; seeks a map &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; between &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y\)&lt;/span&gt; such that (a) &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; minimizes some mapping cost (in Monge's initial problem a distance between points), and (b) &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; satisfies &lt;strong&gt;mass conservation properties&lt;/strong&gt;: all the mass in &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; should be in the set &lt;span class="math"&gt;\(\Gamma[x_i]\)&lt;/span&gt;: &lt;span class="math"&gt;\(\forall j, \sum_{i  | \Gamma[x_i] = y_j} a_i = b_j\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;This problem can also be formulated in continuous spaces, with punctual masses &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; changed into measures &lt;span class="math"&gt;\(\mu_X\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mu_Y\)&lt;/span&gt;. &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; is then constrained to minimize the total transportation cost  &lt;span class="math"&gt;\(\int_X \mu_X(x) c(x, \Gamma(x)) dx\)&lt;/span&gt; under the constraint that for all measurable sets &lt;span class="math"&gt;\(B\)&lt;/span&gt; &lt;span class="math"&gt;\(\int_B \mu_Y(y)dy = \int_{\Gamma^{-1}(B)} \mu_X(x) dx\)&lt;/span&gt;. [Any &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; realizing this is a &lt;em&gt;push forward&lt;/em&gt; mapping, denoted &lt;span class="math"&gt;\(\Gamma_{\#} \mu_X = \mu_Y\)&lt;/span&gt;.]&lt;/p&gt;
&lt;p&gt;In discrete spaces a related problem is the classical combinatorial assignment problem, which corresponds to &lt;span class="math"&gt;\(m=n\)&lt;/span&gt; and all masses equal to one. In this case, the optimal mapping is solved with the &lt;a href=""&gt;Hungarian wedding algorithm&lt;/a&gt; in &lt;span class="math"&gt;\(O(n^3)\)&lt;/span&gt;. Such methods have been used to perform heuristic word alignments in parallel sentences, assuming some matching cost (eg.\ pointwise mutual information or any reasonnable cooccurrence measure) between points. For multilingual embeddings, we will have &lt;span class="math"&gt;\(X\)&lt;/span&gt; as the set of embeddings in one language, and &lt;span class="math"&gt;\(Y\)&lt;/span&gt; the sets of embeddings in another language, and we will need to map one to the other. A nice property is they no longer will need to have the same size.&lt;/p&gt;
&lt;p&gt;The relaxation by Kantorovitch relaxes the assumption that &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; is a mapping and consider probabilistic couplings instead - which means that the masses in each &lt;span class="math"&gt;\(x\)&lt;/span&gt; can be split to be distributed to some &lt;span class="math"&gt;\(y\)&lt;/span&gt;. This yields a probabilistic assignment problem, again subject to marginal constraints.&lt;/p&gt;
&lt;h2&gt;The transport problem&lt;/h2&gt;
&lt;h3&gt;Basics&lt;/h3&gt;
&lt;p&gt;In discrete spaces, we define &lt;em&gt;transportation plans&lt;/em&gt; &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; as follows:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\Gamma: (d \times d) \text{ matrix with coefficients in } \R_+
U(p,q) = \{\Gamma \in \R_+^{d\times{}d}, \Gamma I_d = p, \Gamma^T I_d=q \}
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(U(p,q)\)&lt;/span&gt;, the set of matrices satisfying the constraints, is a bounded set with &lt;span class="math"&gt;\(m+n\)&lt;/span&gt; linear equality constraints. This defines a convex polytope. Another way to look at &lt;span class="math"&gt;\(U(p,q)\)&lt;/span&gt; is that they contain probability distributions with fixed marginals.&lt;/p&gt;
&lt;p&gt;Recall that the Frobenius norm between matrices between matrices &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; and &lt;span class="math"&gt;\(M\)&lt;/span&gt; is defined as: &lt;span class="math"&gt;\(&amp;lt;\Gamma,M&amp;gt; = \operatorname{Tr}(\Gamma^T M) = \sum_{i,j} \Gamma_{ij} M_{ij}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(M\)&lt;/span&gt; be a cost matrix satisfying the following properties &lt;span class="math"&gt;\(M_{ij} = 0\)&lt;/span&gt; iff &lt;span class="math"&gt;\(i=j\)&lt;/span&gt;, &lt;span class="math"&gt;\(M_{ij} =M_{ji}\)&lt;/span&gt; (symmetry), and triangular inequality &lt;span class="math"&gt;\(\forall i,j,k M_{i,k} \lt M_{i,j} + M_{j,k}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; is in &lt;span class="math"&gt;\(U(p,q)\)&lt;/span&gt;, &lt;span class="math"&gt;\(&amp;lt;\Gamma,M&amp;gt;\)&lt;/span&gt; can read as an expectation over &lt;span class="math"&gt;\(\Gamma()\)&lt;/span&gt; of the transportation cost:
&lt;/p&gt;
&lt;div class="math"&gt;$$&amp;lt;\Gamma,M&amp;gt; = \sum_{i,j} \Gamma_{ij} M_{ij} = E_{X,Y \sim P(X,Y)} [M(X,Y)]$$&lt;/div&gt;
&lt;p&gt;
This formulation also serves to define the problem in continuous spaces.&lt;/p&gt;
&lt;h3&gt;Optimal Transport Problem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;(Kantorovitch) Optimal transport  problem (OTP)&lt;/strong&gt; between &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; (for fixed &lt;span class="math"&gt;\(M\)&lt;/span&gt;): Find &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; minimizing &lt;span class="math"&gt;\(&amp;lt;\Gamma,M&amp;gt;\)&lt;/span&gt; subject to marginal constraints &lt;span class="math"&gt;\(\Gamma{} \in U(p,q)\)&lt;/span&gt;. The solution is denoted &lt;span class="math"&gt;\(\operatorname{OTP^_*}(p,q)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This is a tight relaxation of the Monge problem. When the marginal are uniform distributions over sets of the same size, the solution to the OTP is a one-to-one mapping - no other solution is better (proof in [(PC, p15)]).&lt;/p&gt;
&lt;p&gt;OTP is a linear problem (linear objective with linear constraints) that can be solved with the simplex algorithm or variants. The solution is a vertex of the polytope (cannot be an interior point).&lt;/p&gt;
&lt;h3&gt;Distance induced by the OTP&lt;/h3&gt;
&lt;p&gt;The solution to the OTP &lt;span class="math"&gt;\(d_M(p,q)^*\)&lt;/span&gt; is a distance between marginal distributions subject to &lt;span class="math"&gt;\(M\)&lt;/span&gt; being the positive exponent of a distance matrix.&lt;/p&gt;
&lt;p&gt;More precisely, if &lt;span class="math"&gt;\(M=D^\alpha\)&lt;/span&gt;, where &lt;span class="math"&gt;\(D\)&lt;/span&gt; is a distance matrix, then &lt;span class="math"&gt;\(W_{D,\alpha}(p,q) = \operatorname{OTP^*}(p,q)^{1/\alpha}\)&lt;/span&gt; is a distance between &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Symmetry is obvious, &lt;span class="math"&gt;\(Diag(p)\)&lt;/span&gt; transports &lt;span class="math"&gt;\(p\)&lt;/span&gt; to itself at cost 0 assuming &lt;span class="math"&gt;\(M_{i,i} =0\)&lt;/span&gt;, likewise if &lt;span class="math"&gt;\(M(u,v) &amp;gt;0\)&lt;/span&gt; for &lt;span class="math"&gt;\(u \neq v\)&lt;/span&gt;, then the transport cost is strictily positive when &lt;span class="math"&gt;\(p \neq q\)&lt;/span&gt;, transitivity must hold as we can move &lt;span class="math"&gt;\(p\)&lt;/span&gt; to &lt;span class="math"&gt;\(p'\)&lt;/span&gt; with &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt;, &lt;span class="math"&gt;\(p'\)&lt;/span&gt; to &lt;span class="math"&gt;\(p''\)&lt;/span&gt; with &lt;span class="math"&gt;\(\Gamma'\)&lt;/span&gt;, so the cheapest way to transport &lt;span class="math"&gt;\(p\)&lt;/span&gt; to &lt;span class="math"&gt;\(p"\)&lt;/span&gt; must be cheaper than this. The proof in the discrete case is in &lt;a href=""&gt;PC, p20&lt;/a&gt;; for the continuous case we need the glue (sic) lemma.&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(0&amp;lt;p&amp;lt;1\)&lt;/span&gt;, this is no longer true, but need to look at &lt;span class="math"&gt;\(W_{D,\alpha}(p,q)^{\alpha}\)&lt;/span&gt; to get a distance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Duality&lt;/strong&gt;
The dual problem is to find the maximum of &lt;span class="math"&gt;\(E_{\mu_X}(\phi) + E_{\mu_Y}(\psi)\)&lt;/span&gt; such that &lt;span class="math"&gt;\(\phi(x) + \psi(y) \lt M(x,y)\)&lt;/span&gt;. This is can be seen by writing the primal problem in matrix form (&lt;span class="math"&gt;\(min_x c^t x \text{ st. } Ax = b\)&lt;/span&gt;) from which we can identify &lt;span class="math"&gt;\(b\)&lt;/span&gt; to the concatenation of &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt;, which yields the dual form &lt;span class="math"&gt;\(max_y b^Ty \text{ st. } A^Ty =\lt c\)&lt;/span&gt;. This shows that the objective has the form of a sum of two expectations over &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The proof in the discrete case is in [PC, p24] and relies on the theory of Lagrange multipliers (in the discrete case, &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; and &lt;span class="math"&gt;\(\psi\)&lt;/span&gt; &lt;strong&gt;are&lt;/strong&gt; the Lagrange multipliers).&lt;/p&gt;
&lt;p&gt;There is may be a telling analogy. Look at &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; of a price over just moving &lt;span class="math"&gt;\(X\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\psi\)&lt;/span&gt; of just moving &lt;span class="math"&gt;\(Y\)&lt;/span&gt;, such that the price of both is no greater than the transportation cost, then the sum of these two is reaches its max at a value that is also attained by an optimal transportation plan. This is discussed at length in [PC, p24-25].&lt;/p&gt;
&lt;h2&gt;Wasserstein distances&lt;/h2&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt; is a metric space and &lt;span class="math"&gt;\(M(x,y) = D^p(x,y)\)&lt;/span&gt; for some &lt;span class="math"&gt;\(p \ge 1\)&lt;/span&gt;, the induced distance (or rather its &lt;span class="math"&gt;\(p\)&lt;/span&gt;^th) root is the &lt;strong&gt;p-Wasserstein distance&lt;/strong&gt;. This is an instance of the more general Earth Mover Distances, which are defined over arbitrary distances.&lt;/p&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(p=1\)&lt;/span&gt; the [Kantorovitch-Rubinstein duality theorem] states that:
&lt;/p&gt;
&lt;div class="math"&gt;$$
W(p,q) = \sup_{\phi\in F_L} E_{X \sim p} \phi(X) - E_{Y\sim q} \phi(Y)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(F_L\)&lt;/span&gt; is the class of all 1-Lipschitz functions on &lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt;.[#klipschitz] In other words, distance between distributions are defined based on the maximal difference of expectations for all variation-bounded functions.&lt;/p&gt;
&lt;p&gt;A &lt;a href="https://vincentherrmann.github.io/blog/wasserstein/"&gt;detailed proof&lt;/a&gt; shows that these results derives from an analysis of the dual form, where the constraints immediately yields that the optimal of the dual is such that &lt;span class="math"&gt;\(\forall i \phi(x_i) +\psi(x_i) \lt 0\)&lt;/span&gt;. In fact, in can even be shown that given the positivity of &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; the optimum is such that &lt;span class="math"&gt;\(\phi(x_i) +\psi(x_i) = 0\)&lt;/span&gt;, when we add the boundedness constraints.&lt;/p&gt;
&lt;p&gt;These definitions generalize to the continuous case, where Wasserstein distances are very important because they define &lt;strong&gt;&lt;em&gt;weak&lt;/em&gt;&lt;/strong&gt; distances between measures, which can be used to redefine fundamental notions such as &lt;strong&gt;convergence&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Entropic relaxation and fast OTP computation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Sinkhorn distances:&lt;/strong&gt; &lt;span class="math"&gt;\(\forall p,q, \forall \Gamma \in U(p,q), H(\Gamma) \lt H(p) + H(q) = H(pq^T)\)&lt;/span&gt;, so the inegality is tight.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(U_\alpha = \{P \in U(p,q) \text{ such that } \operatorname{KL}(P||rc^T) \lt \alpha \}\)&lt;/span&gt;, (&lt;span class="math"&gt;\(\alpha &amp;gt; 0\)&lt;/span&gt; otherwise this is meaningless),
is the set of transport plans whose entropy is within &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; of the independent plan &lt;span class="math"&gt;\((p^Tq)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Sinkhorn distance: &lt;span class="math"&gt;\(d_{M,\alpha} = min_{ \Gamma \in U_\alpha} &amp;lt;\Gamma,M&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So this is the original transportation with additional entropic constraints on &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; to keep &lt;span class="math"&gt;\(\Gamma\)&lt;/span&gt; smooth (&lt;span class="math"&gt;\(p^Tq\)&lt;/span&gt;) has no nonzero value.&lt;/p&gt;
&lt;p&gt;New problem (for &lt;span class="math"&gt;\(\lambda &amp;gt;0\)&lt;/span&gt;), which yields the dual Sinkhorn divergence:
&lt;/p&gt;
&lt;div class="math"&gt;$$
d_M^{\lambda}(p,q) = \min_{\Gamma \in U(p,q)} &amp;lt;\Gamma,M&amp;gt; - \frac{1}{\lambda} h(\Gamma)
$$&lt;/div&gt;
&lt;p&gt;
For each value of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, &lt;span class="math"&gt;\(\exists \lambda d_M^\lambda(p,q) = d_M^\alpha(p,q)\)&lt;/span&gt; (duality theory). It can be found by varying &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; monotonously -- the entropy of &lt;span class="math"&gt;\(h(\Gamma)\)&lt;/span&gt; is between &lt;span class="math"&gt;\(0\)&lt;/span&gt; and &lt;span class="math"&gt;\(\log(d)\)&lt;/span&gt;, so when &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; goes to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, the second term goes to infinity, and &lt;span class="math"&gt;\(h(\Gamma)\)&lt;/span&gt; increases. [I have a problem here]&lt;/p&gt;
&lt;p&gt;Computing dual Sinkhorn divergences results from an old theorem that says that for each positive matrix &lt;span class="math"&gt;\(A\)&lt;/span&gt;, there is a unique pair of diagonal positive matrices such that &lt;span class="math"&gt;\(Diag(u) A Diag(v)\)&lt;/span&gt; is doubly stochastic and can be computed by the Sinkhorn fixed point algorithm:
assuming we start with uniform &lt;span class="math"&gt;\(u\)&lt;/span&gt; and &lt;span class="math"&gt;\(v\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;$$(u,v) &amp;lt;- p ./ Kv, q./K'u,$$&lt;/div&gt;
&lt;p&gt;
with &lt;span class="math"&gt;\(K = exp(-\lambda M)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assuming proper stopping criteria. Solve this for large values of &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; gets you closer to the actual solution of the initial problem (because the entropy term becomes less and less important).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://michielstock.github.io/OptimalTransport/"&gt;Example implementation&lt;/a&gt;
    :::python
    def compute_optimal_transport(M, r, c, lam, epsilon=1e-8):
    """
    Computes the optimal transport matrix and Slinkhorn distance using the
    Sinkhorn-Knopp algorithm&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;cost&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;marginals&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="k"&gt;c&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;marginals&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;lam&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;strength&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;entropic&lt;/span&gt; &lt;span class="n"&gt;regularization&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;convergence&lt;/span&gt; &lt;span class="k"&gt;parameter&lt;/span&gt;

&lt;span class="n"&gt;Outputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;optimal&lt;/span&gt; &lt;span class="n"&gt;transport&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Sinkhorn&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;
&lt;span class="ss"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="err"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;lam&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;normalize&lt;/span&gt; &lt;span class="n"&gt;this&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;
&lt;span class="n"&gt;while&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;reshape&lt;/span&gt; &lt;span class="n"&gt;will&lt;/span&gt; &lt;span class="n"&gt;handle&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;transposition&lt;/span&gt; &lt;span class="n"&gt;operations&lt;/span&gt;
    &lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;c&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)).&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A computational analysis is in http://papers.nips.cc/paper/6792-near-linear-time-approximation-algorithms-for-optimal-transport-via-sinkhorn-iteration&lt;/p&gt;
&lt;h2&gt;Wasserstein distance as a cost function - Wasserstein GANs&lt;/h2&gt;
&lt;h3&gt;The model&lt;/h3&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(p\)&lt;/span&gt; be the empirical distribution and &lt;span class="math"&gt;\(q\)&lt;/span&gt; the model distribution. For fixed &lt;span class="math"&gt;\(M\)&lt;/span&gt; (eg. the usual metric in a metric space), a natural objective would be to make &lt;span class="math"&gt;\(q\)&lt;/span&gt; as close as &lt;span class="math"&gt;\(p\)&lt;/span&gt; as possible, and to minimize the Wasserstein distance. This is what WGAN do for a class of generative models where we have observed data points distributed under &lt;span class="math"&gt;\(p\)&lt;/span&gt;, and generated data points that result from a 2-step procedure where we first sample &lt;span class="math"&gt;\(z \sim q\)&lt;/span&gt; (eg. a white noise), then generate deterministically &lt;span class="math"&gt;\(x\)&lt;/span&gt; through &lt;span class="math"&gt;\(x=g_\theta(z)\)&lt;/span&gt; (eg. a neural network for instance). We would like to find the optimal &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; minimizing the distance between distributions &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(f_\theta(q)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;or equivalently:
&lt;/p&gt;
&lt;div class="math"&gt;$$
W(p,q) = \sup_{f \in F_L} E_{X \sim p} f(X) - E_{x\sim q} f(Y)
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(F_L\)&lt;/span&gt; is the class of all 1-Lipschitz functions from &lt;span class="math"&gt;\((\mathcal{X},d)\)&lt;/span&gt; into &lt;span class="math"&gt;\(\R\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As &lt;span class="math"&gt;\(x=g_\theta(z)\)&lt;/span&gt;, this program boils down to minimizing
&lt;/p&gt;
&lt;div class="math"&gt;$$
W(p, p_\theta) = \sup_{\phi \in F_L} E_{X \sim p} \phi(X) - E_{Z\sim q} \phi(f_\theta(Z))
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(F_L\)&lt;/span&gt; is the class of all 1-Lipschitz functions on &lt;span class="math"&gt;\((\mathcal{X},d)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assume we have a set of 1-Lipschitz (or &lt;span class="math"&gt;\(K\)&lt;/span&gt;-Lipschitz) functions parameterized by &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, we could approach &lt;span class="math"&gt;\(W(p,p_\theta\)&lt;/span&gt;) as
&lt;/p&gt;
&lt;div class="math"&gt;$$
    max_{\lambda} E_{X \sim p} \phi_\lambda(X) - E_{Z\sim q} \phi_\lambda (g_\theta(Z))
$$&lt;/div&gt;
&lt;p&gt;Bar some technical conditions, Wasserstein GANs will:
1. Find a appropriate &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; to compute the Wasserstein distance
2. Given this &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, minimize the distance in &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. The important mathematical point here is that we can differentiate the Wasserstein distance wrt. &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; as &lt;span class="math"&gt;\(-E_{z \sim q} \nabla_\theta \phi_\lambda(g_{\theta}(z))\)&lt;/span&gt;
3. make sure &lt;span class="math"&gt;\(f_\lambda\)&lt;/span&gt; remains Lipschitz, this is achieved in the original paper by clipping the weights in a fixed &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional box after each update.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;will roughly correspond to optimizing the GAN's critic / discriminator - find a &lt;span class="math"&gt;\(\phi_\lambda\)&lt;/span&gt; whose expectation under &lt;span class="math"&gt;\(p\)&lt;/span&gt; is different from the expectation under &lt;span class="math"&gt;\(p_{\theta}\)&lt;/span&gt;; 2. then updates &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; to reduce this distance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Technical details are in the original publication by &lt;a href="https://arxiv.org/abs/1704.00028"&gt;(Arjosky et al, 2017)&lt;/a&gt;, easier reads are &lt;a href="https://mindcodec.ai/2018/09/23/an-intuitive-guide-to-optimal-transport-part-ii-the-wasserstein-gan-made-easy/"&gt;this post&lt;/a&gt; or &lt;a href="https://www.alexirpan.com/2017/02/22/wasserstein-gan.html"&gt;this one&lt;/a&gt;. The weight clipping part seems to be an issue, and alternatives exist for instance https://arxiv.org/abs/1704.00028.&lt;/p&gt;
&lt;h3&gt;Implementing WGANs&lt;/h3&gt;
&lt;p&gt;The basic implementation would do the following:
   initialize &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;
   for i in range(I):
       while(! not convergence):
           sample &lt;span class="math"&gt;\(x_1 \dots x_n\)&lt;/span&gt; (data points), sample &lt;span class="math"&gt;\(z_1 \dots z_m\)&lt;/span&gt; (generated samples)
       compute &lt;span class="math"&gt;\(F = \frac{1}{n} \phi_{\lambda} (x_i) - \frac{1}{m} \phi_{\lambda} (g_\theta(z_i))\)&lt;/span&gt;
           compute &lt;span class="math"&gt;\(\nabla_\lambda F\)&lt;/span&gt;
           update &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;
           &lt;span class="math"&gt;\(\operatorname{clip}(\lambda)\)&lt;/span&gt;
       compute &lt;span class="math"&gt;\(\nabla_\theta\)&lt;/span&gt;(g) = - \frac{1}{m} \sum \nabla_\theta (\phi_\lambda(g_{\theta}(z_i)))&lt;span class="math"&gt;\(
       update $\theta\)&lt;/span&gt;
   done&lt;/p&gt;
&lt;p&gt;Rq. In the sampling step, does it matter to sample the same number of points ? In the second estimation, can we reuse the same data points ? Does it matter ? We can have more of one than of the other, that would yield better estimates of the expectation.&lt;/p&gt;
&lt;h3&gt;Application to multilingual embeddings&lt;/h3&gt;
&lt;p&gt;The Wassertstein-GAN model is used to trained multilingual embeddings in a supervised fashion in &lt;a href="https://zmlarry.github.io/publications/emnlp2017.pdf"&gt;(Meng Zhang et al, Proc EMNLP 2017)&lt;/a&gt;. In this application, the setting is a bit different from the typical WGAN as the starting point is a pair of sets of monolingual embeddings &lt;span class="math"&gt;\(X_S\)&lt;/span&gt; and &lt;span class="math"&gt;\(X_T\)&lt;/span&gt; in respectively the source and target languages. We still assume that we would like to find a transform from one into the other such that the Wasserstein distance between &lt;span class="math"&gt;\(X_S\)&lt;/span&gt; and &lt;span class="math"&gt;\(g_\theta(X_T)\)&lt;/span&gt;. The authors seem to closely follow the approach of &lt;a href=""&gt;(Arjosky et al, 2017)&lt;/a&gt;, and do not need to pair source and target words in the first step (computation of the Wasserstein distance). The second part only uses one language and also no pairing.&lt;/p&gt;
&lt;h3&gt;Follow-up Questions:&lt;/h3&gt;
&lt;p&gt;1) the basic procedure is asymmetric, and does not require supervision. Which is very nice, but probably not sufficiently put forward.
Alternative approaches assume supervision - can we use it ? This would mean more constraints on the transportation map.&lt;/p&gt;
&lt;p&gt;The authors mention heuristic sparsification of the transportation matrix. I am assuming that changing the regularizer from entropic to L^2 would do exactly this. We would have a extra learnable parameter to control the sparsity level.&lt;/p&gt;
&lt;p&gt;The problem of finding the cost matrix given &lt;span class="math"&gt;\(c\)&lt;/span&gt; is also interesting. This is an inverse problem studied for instance in [https://jmlr.csail.mit.edu/papers/volume20/18-700/18-700.pdf], and it may be somehow more interesting than the initial problem, provided we express the distance function as &lt;/p&gt;
&lt;p&gt;2) the system does not really use masses, whereas we could fairly easily weight words with their frequencies or log-frequencies, and use this in sampling. How should we do this ? use unigram distributions when sampling ? That seems to be the minimum. Would the effect move frequent word -&amp;gt; frequent word ?&lt;/p&gt;
&lt;p&gt;3) Could we do it with a word model ? That is without computing embeddings for a fixed set ? The answer is yes and if we have a way to generate character strings, we could do it much more easily. Think about it.&lt;/p&gt;
&lt;p&gt;4) How good would this be for word alignments ?&lt;/p&gt;
&lt;p&gt;5) How to recover the transportation Map ? Given &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, we have two sets of points, and a function &lt;span class="math"&gt;\(G\)&lt;/span&gt; that maps one set to the other. One would then need to solve the primal problem (or a relaxed version) to get the transportation map. &lt;/p&gt;
&lt;p&gt;6) applications - MT Evaluation with METEOR like stuff&lt;/p&gt;
&lt;p&gt;Additional readings on the same issue (precursor):
- Meng Zhang, Yang Liu, Huanbo Luan, Yiqun Liu, and Maosong Sun. Inducing Bilingual Lexica From Non-Parallel Data With Earth Mover's Distance Regularization. In Proceedings of COLING, 2016. [paper] --&amp;gt; https://zmlarry.github.io/publications/coling2016.pdf
- Meng Zhang, Yang Liu, Huanbo Luan, Maosong Sun, Tatsuya Izuha, and Jie Hao. Building Earth Mover's Distance on Bilingual Word Embeddings for Machine Translation. In Proceedings of AAAI, 2016. [paper] --&amp;gt; https://zmlarry.github.io/publications/aaai2016.pdf
- Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Earth Mover's Distance Minimization for Unsupervised Bilingual Lexicon Induction. In Proceedings of EMNLP, 2017. [paper][code] --&amp;gt; https://zmlarry.github.io/publications/emnlp2017.pdf
And follower:
- Joulin Grave Berthet http://proceedings.mlr.press/v89/grave19a.html&lt;/p&gt;
&lt;p&gt;Side note on the Procrutes part
La partie Procrustes est plus claire, même si il manque encore des détails à bien comprendre. Ce que le Procrustes normal supervise est une matrice binaire remplacée dans l'équation par une matrice de transport. Du point de vue mathématique, on cherche G une rotatation telle que
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
  G &amp;amp; = \argmin_G \sum_{u\in V_S, v \in V_T} T_{u,v} \| u G - v \l^2 \\
    &amp;amp;  = \argmin_G \sum_{u\in V_S, v \in V_T} T_{u,v}    u GG^Tu  + v^Tv - 2 uGv^T \\
    &amp;amp; = \argmax_G \sum_{u\in V_S, v \in V_T} T_{u,v}    uGv^T \\
    &amp;amp; = \argmax_G \operatorname{Trace}( T UGV^T) \\
      &amp;amp; = \argmax_G \operatorname{Trace}(V^T T UG) \\
\end{align}&lt;/div&gt;
&lt;p&gt;
On utilise la SVD de &lt;span class="math"&gt;\(V^T T U = A \Sigma B^T\)&lt;/span&gt; et les propriétés de la trace pour conclure que le minimum est aussi celui de &lt;span class="math"&gt;\(\operatorname{Trace}(A \Sigma B^TG) =\operatorname{Trace}(\Sigma B^TGA)\)&lt;/span&gt; qui est atteint quand &lt;span class="math"&gt;\(B^TGA\)&lt;/span&gt; est l'identité, soit &lt;span class="math"&gt;\(G=BA^T\)&lt;/span&gt;. On retrouve le problème de Procrustes habituel quand &lt;span class="math"&gt;\(T_{u,v}\)&lt;/span&gt; est la matrice diagonale qui supervise les appariemments mot à mot.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;More thoughts on (3): if we have a word generating model eg
w \sample unigram (char)
g(w) = fastext(w)
then I can try to do the same. This has less impact&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://openreview.net/pdf?id=HkL7n1-0b"&gt;Wasserstein auto-encoders&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The crux of the argument is to study the case where the model &lt;/p&gt;
&lt;h2&gt;Sources and references&lt;/h2&gt;
&lt;h4&gt;G.Peyré + M.Cuturi handbook&lt;/h4&gt;
&lt;p&gt;Henceforth &lt;a href="https://arxiv.org/abs/1803.00567"&gt;PC&lt;/a&gt;, very complete, contains the basic proofs and necessary material to understand most works.&lt;/p&gt;
&lt;h4&gt;A nice blog from Raboud scholars (with code) that helps to explain Wasserstein GANs:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mindcodec.ai/2018/09/19/an-intuitive-guide-to-optimal-transport-part-i-formulating-the-problem/"&gt;basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mindcodec.ai/2018/09/23/an-intuitive-guide-to-optimal-transport-part-ii-the-wasserstein-gan-made-easy/"&gt;WGANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mindcodec.ai/2018/10/01/an-intuitive-guide-to-optimal-transport-part-iii-entropic-regularization-and-the-sinkhorn-iterations/"&gt;Regularizing OTP: Sinkhorn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Mathematical fundations&lt;/h3&gt;
&lt;p&gt;http://www.math.cmu.edu/~mthorpe/OTNotes&lt;/p&gt;
&lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;[#lipschitz] Note that when &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; is K-Lifschitz, &lt;span class="math"&gt;\(\frac{phi}{K}\)&lt;/span&gt; is 1-Lifschitz, so we can actually generalize somehow the next results.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Multilinguality, Embeddings"></category></entry><entry><title>La TA "à parité" avec l'humain ? Ca se discute.</title><link href="https://fyvo.github.io/posts/2020/sep/30/la-ta-a-parite-avec-lhumain-ca-se-discute.html" rel="alternate"></link><published>2020-09-30T00:00:00+02:00</published><updated>2020-09-30T00:00:00+02:00</updated><author><name>François Yvon</name></author><id>tag:fyvo.github.io,2020-09-30:/posts/2020/sep/30/la-ta-a-parite-avec-lhumain-ca-se-discute.html</id><summary type="html">&lt;p&gt;'Plusieurs articles récents annoncent que la traduction automatique serait parvenue à "parité avec l'humain". Cette affirmation s'appuie sur un protocole expérimental discutable, et fait l'objet de sérieuses critiques au sein de la communauté. Ce post présente ces résultats, et explique pourquoi de telles affirmations doivent effectivement prises avec des pincettes.'&lt;/p&gt;</summary><content type="html">&lt;p&gt;La &lt;a href=""&gt;parité&lt;/a&gt; est un sujet délicat lorsqu'on s'intéresse à l'égalité entre genres. La parité qui m'intéresse ici n'est pas moins délicate à évaluer, il s'agit de celle entre l'humain et la machine, que je vais aborder par le prisme de la traduction automatique (TA).&lt;/p&gt;
&lt;p&gt;À intervalles réguliers, on entend que grâce aux progrès de l'intelligence artificielle, la machine est maintenant "à parité" (ou presque) avec l'humain dans l'accomplissement de tâches de plus en plus complexes, comme la &lt;a href="https://ieeexplore.ieee.org/abstract/document/8049322"&gt;transcription vocale de parole conversationnelle&lt;/a&gt;, ou &lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-66429-3_51"&gt;la transcription vocale dans le domaine biomédical&lt;/a&gt;. La traduction automatique n'est pas de reste et l'avènement de systèmes de TA égalant l'humain, annoncée dans &lt;a href="https://research.google/pubs/pub45610/"&gt;cet article de Google&lt;/a&gt;, est proclamée dans plusieurs travaux récents, dont l'un publié cette année dans la revue &lt;a href="http://www.nature.com"&gt;Nature&lt;/a&gt; &lt;a href="https://www.nature.com/articles/s41467-020-18073-9"&gt;(Popel et al, 2020)&lt;/a&gt;, ce qui ne manquera pas de donner du poids à cette affirmation.&lt;/p&gt;
&lt;p&gt;Pourquoi faut-il prendre de telles annonces avec des pincettes ? Principalement pour trois raisons, qui sont analysées en détail dans une série de travaux (&lt;a href="https://arxiv.org/abs/1808.10432"&gt;(Toral et al, 2018)&lt;/a&gt;;&lt;a href="https://arxiv.org/abs/2005.05738"&gt;(Toral, 2019)&lt;/a&gt;) , le &lt;a href="https://jair.org/index.php/jair/article/view/11371"&gt;dernier de la liste&lt;/a&gt; en dressant une synthèse très claire. Avant de résumer leurs arguments, commençons par rappeler comment les évaluations de la TA sont menées aujourd'hui au sein de la communauté - en particulier dans le cadre des campagnes d'évaluations annuelles réalisées sous l'égide de la &lt;a href="http://statmt.org/wmt20"&gt;Conférence sur la Traduction Automatique&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Evaluer les traductions automatiques&lt;/h2&gt;
&lt;p&gt;Si les métriques automatiques (BLEU, NIST ou Meteor) ont été utilisées pour les premières campagnes d'évaluation, elles ont été rapidement abandonnées au profit d'évaluations manuelles. La méthode qui s'est progressivement imposée consiste ainsi à demander à des juges humains d'attribuer une valeur de 1 à 100 à exemples de traductions automatiques. Pour guider ce jugement, on leur présente également une traduction de référence réalisée par un humain, ainsi qu'une consigne simple "Sur une échelle de 1 à 100, dire à quel point vous êtes d'accord avec l'affirmation que le texte en noir (la traduction) exprime le sens du texte grisé (la traduction de référence)". Cette tâche est très simple à réaliser et ne demande pas de compétence particulière en langue source. Elle permet, par aggrégation (et normalisation) de multiples évaluations réalisées indépendamment par de multiples évaluateurs, de donner une note globale à chaque système et ainsi de les ordonner du meilleur au pire. Une variante consiste à présenter la phrase source à la place de la traduction de référence; elle requiert des évaluateurs bilingues et est donc plus difficile à mettre en oeuvre. L'interface d'évaluation pour des évaluateurs bilingues est reproduite ci-dessous (figure tirée de &lt;a href="https://www.aclweb.org/anthology/W19-5301.pdf"&gt;(Barrault et al, 2019)&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img alt="L'interface d'évaluation du système &amp;quot;Appraise&amp;quot;" src="https://fyvo.github.io/images/appraise-sentence-source-nocontext.png"&gt;&lt;/p&gt;
&lt;p&gt;Dans les premières expériences dont il est question ici, qui portent sur les paires de langues anglais-chinois &lt;a href="https://arxiv.org/abs/1803.05567"&gt;(Hassan et al, 2018)&lt;/a&gt;, et tchèque-anglais &lt;a href="http://statmt.org/wmt18/pdf/WMT028.pdf"&gt;(Bojar et al, 2018)&lt;/a&gt;, l'affirmation de parité de la machine avec l'humain a été obtenue en comparant avec cette méthode le "score" global d'un traducteur ou d'une équipe de traducteurs avec une TA et en n'observant que ces scores n'étaient pas significativement différents.[^1]&lt;/p&gt;
&lt;p&gt;Du point de vue méthodologique, les mesures que ces travaux mettent en exergue sont conformes aux pratiques de la communauté. Elles ne permettent pas de conclure à la "parité" pour autant, pour au moins trois raisons qui sont longuement discutées dans &lt;a href="https://jair.org/index.php/jair/article/view/11371"&gt;(Läubli et al, 2020)&lt;/a&gt; et qui ont trait aux détails du protocole expérimental.&lt;/p&gt;
&lt;h2&gt;L'évaluation de phrases isolées&lt;/h2&gt;
&lt;p&gt;Les systèmes de TA, dans leur majorité, traduisent des phrases isolées. Est-il pour autant raisonnable de demander aux juges d'évaluer des phrases isolées ? La réponse est plutôt non, et les erreurs de la machine apparaissent plus clairement quand les évaluateurs ont accès aux phrases qui précèdent et suivent la phrase évaluée, ce qui conduit à diminuer leur score global. Ce contexte supplémentaire permet par exemple de repérer des des fautes d'accord, des incohérences, etc, qui ne peuvent être détectées à partir d'une phrase isolée. Dans la mesure où l'activité de traducteurs humains ne consiste qu'exceptionnellement à traduire des phrases isolées, pour atteindre la "parité", il faudra que les productions de la machine soit indiscernable d'une traduction humaine pour '''des documents complets'''. Notons qu'il n'existe pas pour l'instant de méthode pour comparer efficacement des traductions de documents entiers - c'est un sujet qui gagne de l'importance au sein de la communauté, en atteste la proposition récente de &lt;a href="https://openreview.net/pdf?id=Hygfmc5U-7"&gt;(Läubli et al, 2018)&lt;/a&gt;; en revanche l'idée d'intégrer un contexte discursif local aux phrases analysées a fait son chemin.
Elle est utilisée dans la dernière campagne d'évaluation organisée dans le cadre de la &lt;a href="http://statmt.org/wmt2019"&gt;conférence WMT 2019&lt;/a&gt; et documentée dans &lt;a href="https://www.aclweb.org/anthology/W19-5301.pdf"&gt;(Barrault et al, 2019)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;En pratique, chaque phrase à évaluer est présentée en faisant également figurer les phrases qui la précèdent et la suivent immédiatement; les phrases sont évaluées dans l'ordre de leur succession dans le texte[^2]. Avec cet ajustement, la "parité avec l'humain" n'est atteinte que pour un seul système et une direction de traduction, alors que l'évaluation sans contexte laisserait penser que trois directions de traduction (anglais-allemand dans les deux directions, russe vers anglais) sont concernées. &lt;/p&gt;
&lt;h2&gt;Les traductions de référence&lt;/h2&gt;
&lt;p&gt;Dans le protocole décrit ci-dessus, les évaluateurs ont accès à des traductions de référence qui leur permettent d'apprécier la qualité des traductions produites. Dans le cadre usuel des évaluations de la TA, ces références sont souvent produites par des locuteurs compétents de S et de T, mais rarement par des locuteurs natifs. Pourtant il apparait, si l'on améliore la qualité de ces références, que les évaluateurs se montrent plus critiques avec les traductions de la machine, comme si en augmentant le contraste, on mettait mieux en lumière les approximations de la TA. Pour pouvoir conclure à la parité, il faudrait comparer les productions de la machine à des traductions humaines dont la qualité est contrôlée. Traduire est un métier, et bien traduire n'est pas donné à tout locuteur, même bilingue.&lt;/p&gt;
&lt;p&gt;Paradoxalement, l'usage d'un protocole d'évaluation "monolingue" semble jouer en défaveur des traductions humaines. Dans cette configuration, les traductions ressemblant à la référence sont jugées bonnes - indépendemment de leur qualité intrinsèque -, celles qui s'en éloignent sont inversement jugées mauvaise. Or il existe entre traducteurs différents une variabilité telle que la seconde situation est, dans les faits, assez commune, et conduit à une dégradation des évaluations des traductions humaines. Ce biais est en particulier documenté dans &lt;a href=""&gt;(Bentivogli et al, 2018)&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2005.05738.pdf"&gt;(Toral, 2019)&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Le choix des évaluateurs&lt;/h2&gt;
&lt;p&gt;Pour évaluer des traductions entre les langues S et T, il faut des évaluateurs capables de le faire. Est-ce donné à tout locuteur de la langue T ? Des travaux antérieurs, conduits avec des systèmes statistiques, tendent à montrer que oui, ou plus précisément qu'en accumulant des avis "profanes" sur la qualité des traductions, on obtient les mêmes conclusions que si l'on collecte des avis "experts" (d'experts en traduction). Il apparait pourtant que pour les systèmes neuronaux -- dont la qualité apparente (la fluidité de la langue cible), ce résultat n'est plus valable: lorsque l'on consulte de "vrais" experts (par exemple des traducteurs professionnels), ils se montrent en moyenne plus critiques que les non-experts pour juger les traductions automatique &lt;a href=""&gt;(Castilho et al, 2017)&lt;/a&gt;. '''Première recommandation:''' il faut des experts traducteurs (encore mieux, des enseignants dans les cursus de traduction) pour juger si oui ou non les traducteurs automatiques les surpassent (comme il faut un champion du monde de Go, et pas un amateur, pour faire reconnaître la supériorité de la machine à ce jeu).&lt;/p&gt;
&lt;h2&gt;Le choix de la tâche&lt;/h2&gt;
&lt;p&gt;Quels sont les textes traduits ?&lt;/p&gt;
&lt;h2&gt;Choix de la source et "traductionnais"[^3]&lt;/h2&gt;
&lt;p&gt;En dépit de leurs faiblesses méthodologiques, il est important de relever que les travaux cités (à voir) avaient pris soin d'éviter un écueil bien documenté, qui a trait à la sélection des phrases à traduire. Un usage ancien consiste à évaluer les deux directions S-&amp;gt;T et T-&amp;gt;S en parallèle, à partir d'un lot de documents de référence constitué pour partie de N documents écrits en S (et traduits en T), et pour l'autre partie, de M documents rédigés en T (et traduits en S). Cette pratique a pour effet de maximiser la rentabilité des traductions humaines, puisque l'on dispose ainsi de M+N documents parallèles pour évaluer chaque direction de traduction. Le problème est qu'une partie des phrases sources utilisées pour évaluer la qualité de la traduction de S vers T sont en fait des traductions (humaines) de T vers S, et sont en fait bien plus faciles à (re)traduire automatiquement que des vraies phrases sources initialement produites en langue S. La raison est que la langue des traductions (le traductionnais) diffère de la "vraie" langue source de multiples manière: lexique simplifié, structures syntaxiques calquées sur la langue source, etc - autant de traits qui la rend plus facile à retraduire. Une analyse de ce phénomène est donné dans &lt;a href="http://www.statmt.org/wmt18/pdf/WMT012.pdf"&gt;(Toral et al, 2018)&lt;/a&gt;, à partir des données &lt;a href="https://arxiv.org/abs/1803.05567"&gt;(Hassan et al, 2018)&lt;/a&gt;, qui évalue la différence de qualité de traduction entre les deux types de phrases à au moins 5 %.&lt;/p&gt;
&lt;h2&gt;L'étude de Nature&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.nature.com/articles/s41467-020-18073-9"&gt;L'étude publié dans Nature&lt;/a&gt; réutilise les données de l'évaluation de 2018 pour la direction anglais vers tchèque pour le système de traduction CUBBITT et cherche à éviter certains des biais listés ci-dessus. En particulier, elle utilise une évaluation en contexte et a recours à un groupe d'évaluateurs experts, dont les jugements complètent ceux des évaluateurs profanes. Elle se distingue par une échelle réduite (de 1 à 10), et par l'évaluation séparée de la fluidité (&lt;strong&gt;&lt;em&gt;fluency&lt;/em&gt;&lt;/strong&gt;) du texte cible généré, et de son adéquation (&lt;strong&gt;&lt;em&gt;adequacy&lt;/em&gt;&lt;/strong&gt;) au texte source[^4]. Ses résultats corroborent en fait largement les observations précédentes: évaluer en contexte permet de mieux distinguer les failles de la traduction automatique; les évaluateurs professionnels sont plus critiques à son égard. Et contrairement à ce qu'annonce le titre (''Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals") de l'article, la traduction automatique est en fait jugée de moindre qualité que la traduction humaine: "CUBBITT outperformed the human reference in adequacy, whereas &lt;strong&gt;&lt;em&gt;the reference was scored better in fluency and overall quality&lt;/em&gt;&lt;/strong&gt; (Supplementary Fig. 7)." Elle contient toutefois plusieurs résultats complémentaires intéressants, notamment (a) la difficulté pour des humains à distinguer les traductions automatiques des traductions humaines; (b) la supériorité de la TA en matière de préservation du sens ("(...) CUBBITT significantly out-performed professional-agency English-to-Czech News translation &lt;strong&gt;&lt;em&gt;in preserving text meaning (translation adequacy). While human translation is still rated as more fluent&lt;/em&gt;&lt;/strong&gt; (...),). Cette observation va à l'encontre l'avis général que la TA neuronale excelle à produire des sorties grammaticalement correctes, au détriment parfois de la préservation du sens.&lt;/p&gt;
&lt;h1&gt;Parité ou pas parité&lt;/h1&gt;
&lt;p&gt;Les progrès de la traduction automatique sont réels, tangibles et transforment le secteur de la traduction professionnelle; ils se diffiusent rapidement et font naitre des usages nouveaux -- par exemple pour l'apprentissage des langues --, gagnant chaque jour de nouveaux utilisateurs à ces technologies. C'est bien le principal, et la question de la parité avec l'humain semble bien secondaire au fond -- même si elle permet des annonces triomphales. Contrairement à certaines capacités cognitives universellement répandues, et que la machine pourrait un jour atteindre ou dépasser, la notion de "performance humaine"  en traduction de veut pas dire grand'chose. Il existe d'excellents humains traducteurs, il en existe aussi de mauvais, mais l'humain qui est de loin le plus répandu est le non-traducteur, qui est donc loin d'être à parité avec des modèles qui ont analysé, pour leur apprentissage, des centaines de millions de phrases parallèles. Par ailleurs, l'exercice de la traduction présuppose toujours un récepteur, pour les besoins duquel on traduit, à chaque fois différemment. Faute, au fond, de pouvoir dire avec quel(s) humain(s), et pour quels usages, on compare la TA, les débats oiseaux sur la parité restent condamnés à tourner en rond.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[1] Les définitions proposées par &lt;a href="https://arxiv.org/abs/1803.05567"&gt;Hassan et al., (2018)&lt;/a&gt; sont les suivantes:
    * Definition 1.If a bilingual human judges the quality of a candidate translation produced by a human to be equivalent to one produced by a machine, then the machine has achieved human parity.
    * Definition 2. If there is no statistically significant difference between human quality scores for a test set of candidate translations from a machine translation  system  and  the  scores  for  the  corresponding human translations then the machine has achieved human parity.&lt;/p&gt;
&lt;p&gt;[2] La taille du contexte discursif à présenter pour réaliser une évaluation juste est discutée récemment dans &lt;a href="https://www.aclweb.org/anthology/2020.lrec-1.461.pdf"&gt;Castilho et al, 2020&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[3] Je traduis ainsi l'anglais "translationese", qui désigne la langue des documents traduits. Le translationnais diffère de la langue cible de la traduction sous de multiples aspects, qui sont décrits et analysés en particulier par Mona Baker.&lt;/p&gt;
&lt;p&gt;[4] L'utilisation de notations séparées pour la fluidité et la préservation du sens est un retour à une pratique ancienne, mais délaissée. Il semblait acquis, au moins pour la génération précédente des systèmes, que ces deux dimensions étaient très liées, et difficiles à évaluer séparément. En particulier, évaluer l'adéquation d'un texte incompréhensible relève souvent de la gageure.&lt;/p&gt;</content><category term="Machine Translation"></category></entry><entry><title>Peer reviewing (in Comp Ling) - Why it is broken and ways to fix it</title><link href="https://fyvo.github.io/posts/2020/sep/30/peer-reviewing-in-comp-ling-why-it-is-broken-and-ways-to-fix-it-en.html" rel="alternate"></link><published>2020-09-30T00:00:00+02:00</published><updated>2020-09-30T00:00:00+02:00</updated><author><name>François Yvon</name></author><id>tag:fyvo.github.io,2020-09-30:/posts/2020/sep/30/peer-reviewing-in-comp-ling-why-it-is-broken-and-ways-to-fix-it-en.html</id><summary type="html">&lt;h2&gt;Reviewing  takes a lot time&lt;/h2&gt;
&lt;p&gt;I review a lot. If review for conferences, workshops, journals. I am also AE for &lt;a href="http://transacl.org"&gt;TACL&lt;/a&gt;, &lt;a href="http://dl.acm.org/journals/csur"&gt;ACM Computing Surveys&lt;/a&gt;, &lt;a href=""&gt;Computer Speech and Language&lt;/a&gt; and &lt;a href="http://www.atala.org/revuetal"&gt;Traitement Automatique des Langues&lt;/a&gt; (mostly in French). Which means that I review too much. Hopefully my employer (&lt;a href="http://www.cnrs.fr"&gt;CNRS&lt;/a&gt;) does not seem …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Reviewing  takes a lot time&lt;/h2&gt;
&lt;p&gt;I review a lot. If review for conferences, workshops, journals. I am also AE for &lt;a href="http://transacl.org"&gt;TACL&lt;/a&gt;, &lt;a href="http://dl.acm.org/journals/csur"&gt;ACM Computing Surveys&lt;/a&gt;, &lt;a href=""&gt;Computer Speech and Language&lt;/a&gt; and &lt;a href="http://www.atala.org/revuetal"&gt;Traitement Automatique des Langues&lt;/a&gt; (mostly in French). Which means that I review too much. Hopefully my employer (&lt;a href="http://www.cnrs.fr"&gt;CNRS&lt;/a&gt;) does not seem to mind. Yet. The situation is getting worse every year, to the point where I can feel&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; -- as many -- that our model is broken. This point was already raised years ago in our community notably by &lt;a href="CL31-4"&gt;K. Church (2005)&lt;/a&gt; and &lt;a href="CL37-1"&gt;I. Mani, (2011)&lt;/a&gt;, as in others. A very long thorough analysis of the issue, and proposals to improve our current practicies is in &lt;a href="https://arxiv.org/pdf/2010.03863.pdf"&gt;A. Rogers and I. Augenstein (2020)&lt;/a&gt;&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;, following a series of &lt;a href="https://hackingsemantics.xyz/"&gt;Blog posts&lt;/a&gt; by the first author. These proposals are mostly intended at maintaining fairness and accountability of the peer review procedure despite the increase of submissions, and I share many of them.[\R&amp;amp;A] My view point is a bit different, though, since I am primarily interested in ways to spare reviewer's time - since at it seems, the most critical resource in the system.&lt;/p&gt;
&lt;p&gt;We review more because there are more scholars and venues and papers, and our field is growing and being more active, and diverse. This is probably a good thing. More scholars could imply more reviewers, so the situation may not be that bad.[^2] Still.&lt;/p&gt;
&lt;p&gt;Another reason why there are more reviews is because there a a growing pressure on scholars to increase publication statistics :to get a enroll in a good PhD, to get a position, to get tenured, to be promoted. &lt;/p&gt;
&lt;p&gt;As my field (Computational Linguistics / Natural Language Processing &amp;gt; Machine Learning &amp;gt; IA) is a very active, there is also a pressure to be the first to [add stuff here], so papers need to have to be published quickly and getting reviews is now part of the writing process. Which could be considered as a good thing as well, reviews can help strengthen papers. &lt;/p&gt;
&lt;p&gt;So for a writer, there is no reason why not send a paper to review: if reviews are good, the paper will be published with the "peer-reviewed" stamp, if they are bad, they can help make the paper better. If this the paper is not good enough for the current deadline, it might work for the next. All this for free. &lt;/p&gt;
&lt;p&gt;For a reviewer, all this combines to make things worse -- more papers to read and reviews to write. All this for free, and most of the time noone will ever know. So one of my reaction as been to cut down on the number of reviews, and stop reviewing (and submitting) papers outside the the core NLP community (no more NIPS, ICML, ICLR, AAAI, IJCAI, etc) for me, even though these conferences also publish a significant lot of NLP related papers.&lt;/p&gt;
&lt;p&gt;For the field as a whole, this has a clear downside: even with more slots and venues, more papers compete for the good spots, which means that: (a) people have less time to review, and the quality of reviews drops (may be also as a conjunction of side effects such as the direct or indirect implication of more junior researchers in the process) (b) paper acceptance is increasingly random and very much depends upon the set of reviewers; (c) good papers get rejected; (d) bad papers can be reviewed multiple times, and waste reviewer time (a); (e) with more "core papers" to review, there is less time to accept reviewing from other, related, fields (applied linguistics, ML, image) - maybe.&lt;/p&gt;
&lt;h2&gt;Why review ?&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Because this is how our field, and more generally how science, works. Ideas, experiments, claims, results, proofs need to be reviewed, verified, criticized and discussed before they can slowly be turn into facts and knowledge. Ideally, we should even aim at full reproducibility. Reviewing is part of our job.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Because this is good for our CV. Being a reviewer, a workshop / area chair, an editor, is good for the CV. It shows recognition and expertise. With more reviews to write, this might not be a very distinctive feature of our resumes. Nonetheless.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Because we can learn from others. Reviewing helps keeping in sync with new ideas in the field. So does following &lt;a href="http://arxiv.org"&gt;arxiv&lt;/a&gt;, sure, but reviewing, in addition, requires to actually read, rather than skim, the papers (well, that is the theory).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In any case writing a long, detailled, well-argued review, remains a pure act of altruism. What you write will not stay; almost noone will read it, and (almost) noone will know about your work. Why bother? Writing bad reviews is so much better. You still improve your CV, at a reduced cost. And (almost) noone will know.&lt;/p&gt;
&lt;h2&gt;The ACL discussion and proposals&lt;/h2&gt;
&lt;p&gt;ACL has opened the discussion regarding a series of short term, as well as long term, proposals to reform the reviewing and publication process. The discussion page is still open. Use it.&lt;/p&gt;
&lt;p&gt;They start their discussion with four problems with the current system, three that I share (see below), and one that I do not. I will start with this one.&lt;/p&gt;
&lt;p&gt;Turnaround time. Quoting the ACL Exec: "A large factor in the incentive to go to arXiv is that turnaround time from when a paper is finished to when it can be made public can be many months, especially when you get several random rejects." So one first comment is that this time issue is related to the first problem identified by the ACL Exec (arxiv and non-blind reviews). If we fix this one (and we can, by having an ACL operated blind archive, a proposal that is starting to gain momentum), may be the turnaround issue will not be that bad. There are multiple deadlines around the year and multiple venues to publish good papers. Most conferences of our field are called "archival" for a reason - the papers they archive are meant to remain relevant for a long time. For papers that cannot wait the extra-month before being released, because they risk to lose their value, &lt;a href="http://arxiv.org"&gt;arxiv&lt;/a&gt; is an excellent choice, but please do not expect a good (and free) peer review. This is in fact my take on ACL Exec's first proposal: a ban on pre-publishing on arxiv - as this breaks blind review. So sad news: good peer review is somehow antoganistic to fast turnaround -- even if we can make our best effort, as &lt;a href="https://transacl.org/ojs/index.php/tacl/about/editorialPolicies#peerReviewProcess"&gt;TACL&lt;/a&gt; has, to reduce the total duration of the reviewing process.&lt;/p&gt;
&lt;p&gt;A possible answer to this might be the launching of &lt;a href="https://2020.emnlp.org/blog/2020-04-19-findings-of-emnlp"&gt;a new publication vehicule&lt;/a&gt; 'Findings of EMNLP' for borderline papers which are in a hurry to be published on a peer-reviewed support. The future will tell whether this initiative will help reduce the number of unpublished papers on arxiv (which is not a concern), as well as the number of resubmitted papers (which is a concern). There are &lt;a href="https://2020.emnlp.org/blog/2020-07-30-findings-acl-response"&gt;good reasons&lt;/a&gt; to support this initiative. &lt;a href="https://arxiv.org/pdf/2010.03863.pdf"&gt;A. Rogers and I. Augenstein (2020)&lt;/a&gt; give (ironically, in a 'Findings' paper) compelling reasons why the exact contrary could well happen.&lt;/p&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;h3&gt;Reduce the number of paper to review: draining down the pipe&lt;/h3&gt;
&lt;p&gt;Ask each author to explicitely endorse their paper(s). It is not rare to see some authors having their name on dozens of papers submitted at a given conference. When reviews are random, this is the most rational course of action: submitting more papers will maximize the expectation of the number of publications. How many papers can one actually author over a short time frame? That number may vary from indivudals to individuals and from teams to teams. There is however a definition of what authoring a paper means. A simple nudge would be to ask each author to read this short text and validate its status as an author. In fact, &lt;a href="https://transacl.org/ojs/index.php/tacl/about/editorialPolicies#custom-0"&gt;our ethical guidelines&lt;/a&gt; focus much more on editors and reviewers than on authors. This is an easy fix.&lt;/p&gt;
&lt;p&gt;Desk reject more papers. Style and presentation issues are a problem. They make papers more difficult to read - and pose reviewers an additional burden: to report typos, style errors etc. A first step would be to have a decent English style checker included in the submission pipeline, with a public bar for automatic rejection. Mentoring could be proposed more systematically to assist paper writing (until the time when everyone can write in her own language). Given than we have already solved Machine Translation and Natural Language Understanding[^3], this seems doable. Likewise for plagiarism and incremental publications. Good journals have such tools. We should as well.&lt;/p&gt;
&lt;p&gt;Better control republication. This is a &lt;a href="http://www.transacl.org"&gt;TACL policy&lt;/a&gt;. Poor papers cannot be resubmitted before a period of time - taken to be long enough for the authors to make the necessary adjustments.&lt;/p&gt;
&lt;p&gt;Distribute fixed submission tokens. This amounts to granting authors a limit of the number of papers they can have reviewed each year. As for the previous proposal on republication, this poses boundaries issues that would be hard to fix given the growing "fluidity" between conferences (LREC - Interspeech ICASSP - ACL - ECAI, IJCAI, AAAI - IMCL, NIPS, AIStats, ICLR, etc.)&lt;/p&gt;
&lt;p&gt;Give more tokens if you review more papers, or if you publish more papers. Likely to create a rich-get-richer effect (in the best case) or a black market of reviewing tokens (in the worst), and yield many discussions about implementation details (how many tokens for team work ? One per author ? Just one to be split among authors ? just the first ? Just the last ?  Just one random author ?). Having one central repository, as proposed by the ACL Exec will enable to compute interesting statistics (max / mean number of submission and review per year and per scholar or institution etc).&lt;/p&gt;
&lt;h3&gt;Improve the reviewing quality (work on the offer side)&lt;/h3&gt;
&lt;p&gt;Incentivize good reviews. This is a proposal of the ACL review committee which recommand to recognize good reviews and reviewers and grant awards &lt;a href="https://www.aclweb.org/adminwiki/index.php?title=Short-Term_Reform_Proposals_for_ACL_Reviewing"&gt;Short-term action number 3&lt;/a&gt;. Which will imply that someone will have to review the reviews -- arguably AC / AEs read review and can evaluate them; but to compare them on fair grounds and distribute valuable rewards will require to build a more robust sytem. Do we have time for this? Conversely, one could think of many ways to also discourage bad rewiews (a prize for the worst reviewer(s), a publication of the worst reviews (with authors names or institution names), etc, etc?). Probably not the road to follow.&lt;/p&gt;
&lt;p&gt;Train reviewers. This is also an important ACL proposal &lt;a href="https://www.aclweb.org/adminwiki/index.php?title=Short-Term_Reform_Proposals_for_ACL_Reviewing"&gt;Short-term action number 4&lt;/a&gt;. Learning to review should be part of any good PhD training program, and a lot of material has already been published, as pointed out in &lt;a href="https://2020.emnlp.org/blog/2020-05-17-write-good-reviews"&gt;this EMNLP 2020 Blog post&lt;/a&gt; or [editors]. Having reviews endorsed by senior staff members could also be a good thing. As long as the right person gets credit for the review. Only selecting reviewers that have been duely trained and prepared for this should be a basic rule of Program Committee building. Should one go one step further and enforce formal requirements (ie. to have a completed PhD degree) ?&lt;/p&gt;
&lt;p&gt;Make reviews public. &lt;a href="https://papers.nips.cc/book/"&gt;NeurIPS&lt;/a&gt; (as of 2013) and &lt;a href="https://iclr.cc/"&gt;ICLR&lt;/a&gt; make their anonymous reviews (as well as author's rebuttal) public forever and I tend to think that this tends to improve reviews (could we test this?). If additionnally reviews are signed, this should nudge reviewers towards paying more attention to what they write, which will last, have more readers, more impact, and may help make new friends (or enemies). Non-blind reviewing systems have other biases that are well documented, so the cure might again be worst than the disease.&lt;/p&gt;
&lt;h2&gt;Political grunts&lt;/h2&gt;
&lt;p&gt;About paid reviews: https://www.the-scientist.com/careers/scientists-publishers-debate-paychecks-for-peer-reviewers-68101?mc_cid=540aef3aa7&amp;amp;mc_eid=b30567b349&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[] Excerpt from the &lt;a href="https://www.jsps.go.jp/english/e-kousei/data/singapore_statement_EN.pdf"&gt;Singapore statement on research integrity&lt;/a&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;'''Authorship''': Researchers should take responsibility for their contributions to all publications, funding applications, reports and other representations of their research. Lists of authors should include all those and only those who meet applicable authorship criteria.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;'''Publication Acknowledgement''': Researchers should acknowledge in publications the names and roles of those who made significant contributions to the research, including writers, funders, sponsors, and others, but do not meet authorship criteria&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;[R&amp;amp;A] There are many points worth discussing in this paper. One statement that I find very disturbing is about the indisputable randomness of reject/accept decisions, which the authors attribute to alledged the power law distribution of quality. I disagree - for reasons developed in [this associated post].&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;EMNLP this year required that for each submitted paper, at least one author had to be a registered candidate reviewer. Suggesting that there was some unbalance between the number of authors and reviewers.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="reviewing"></category><category term="reviewing"></category><category term="ACL"></category></entry><entry><title>Note sur le calcul de la perplexité</title><link href="https://fyvo.github.io/posts/2020/jan/01/note-sur-le-calcul-de-la-perplexite.html" rel="alternate"></link><published>2020-01-01T00:00:00+01:00</published><updated>2020-01-01T00:00:00+01:00</updated><author><name>François Yvon</name></author><id>tag:fyvo.github.io,2020-01-01:/posts/2020/jan/01/note-sur-le-calcul-de-la-perplexite.html</id><summary type="html">&lt;h1&gt;Une perplexité qui laisse perplexe&lt;/h1&gt;
&lt;p&gt;Les modèles de langue continuent de faire l'objet de nombreuses recherches, en particulier parce que les données d'apprentissage sont abondantes, et permettent de pré-entrainer des représentations ou des réseaux qu'il est possible ensuite de détourner à d'autres fin, presque sans supervision. Le &lt;a href="https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper.pdf"&gt;travail récent de …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Une perplexité qui laisse perplexe&lt;/h1&gt;
&lt;p&gt;Les modèles de langue continuent de faire l'objet de nombreuses recherches, en particulier parce que les données d'apprentissage sont abondantes, et permettent de pré-entrainer des représentations ou des réseaux qu'il est possible ensuite de détourner à d'autres fin, presque sans supervision. Le &lt;a href="https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper.pdf"&gt;travail récent de Radford et collègues&lt;/a&gt; en donne une très belle illustration, puisque leur modèle de langue est utilisé (une fois adapté à ces tâches) aussi bien pour la classification de documents que pour le calcul d'équivalences sémantiques. Ce travail est encore étendu dans une nouvelle publication, qui exploite une architecture bien plus profonde, et un bien plus grand nombre de données d'apprentissage. Dans ce dernier travail, les auteurs présentent des performances de leur modèle en termes de perplexité, présentant des résultats qui surpassent de loin l'état de l'art. Ainsi ils obtiennent sur le PennTreeBank [] (table 3) un score de 35.76, alors que le très sérieux travail de Melis et co-auteurs [ ], en 2017, compare une dizaine de modèles de l'état de l'art, avec des scores (Table 1) s'échelonnant entre 86.2 et 58.3, des résultats qui sont encore améliorés par Yang et co-auteurs en 2018 [] pour atteindre 47.7 (Il est possible de faire encore un peu mieux - avec une technique qui utilise des réseaux adversariaux pour améliorer les représentations de mots rares []). &lt;/p&gt;
&lt;p&gt;Note: le travail de [[]] se concentre sur le Giga word et ne produit pas de résultat sur le PTB. En revanche, il utilise des modèles de caractères sur la couche de sortie qui semblent capables de scorer n'importe quelle suite de caractère, c-à-d qu'au lieu de calculer directement &lt;span class="math"&gt;\(P(w=x_1\dots x_n|h)\)&lt;/span&gt; il utilise &lt;span class="math"&gt;\(P(x_1|h)p5x_2|x_1,h)\dots P(eow|x_1\dotsx_t,h)\)&lt;/span&gt;. Quel est l'effet sur le facteur de normalisation ? Ce serait à regarder de près.&lt;/p&gt;
&lt;h1&gt;Qu'est ce que la perplexité ?&lt;/h1&gt;
&lt;p&gt;La perplexité est une mesure de la capacité de prédiction du modèle de langue &lt;span class="math"&gt;\(M\)&lt;/span&gt;, proposée dès les travaux pionniers de Shannon sur les modèles de Markov [@Shannon51prediction]. Elle est le plus souvent définie en relation avec l'entropie croisée de la source &lt;span class="math"&gt;\(H(M) = E_{x_1 \dots X_T \sim S} \log P(x_1 \dots X_T) = \frac{1}{T} \log P_M(x_t | x_{&amp;lt;t})\)&lt;/span&gt; par &lt;span class="math"&gt;\(PP(M) = 2^H(M)\)&lt;/span&gt;, qui présuppose un modèle capable d'assigner une probabilité à toute séquence de mots, typiquement en la décomposant en utilisant la règle de factorisation de la loi jointe comme un produit de lois conditionnelles. Dans les modèles discrets (n-gramme) [], chaque facteur définit une distribution discrète sur un vocabulaire prédéfini, de taille connue &lt;span class="math"&gt;\(|V|\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Une autre manière de trouver ce résultat est d'imaginer un jeu dans lequel il on demande de deviner des mots absents d'un texte en formulant des hypothèses sous la forme d'une distribution de probabilité sur le vocabulaire de sortie. Si le pari correspond à la distribution de probabilité conditionnelle au contexte gauche, on retrouve la forme précédente lorsque l'on prédit successivement les mots de gauche à droite. L'intérêt du jeu de Shannon c'est qu'il autorise à utiliser d'autres modèles (par exemple: qui utilisent le contexte droit, ou bien simultanément le contexte droit et gauche).&lt;/p&gt;
&lt;p&gt;Un des points d'attention dans ces calculs est la taille du vocabulaire. Il est d'une part important de pouvoir produire toutes les séquences possibles, ce qui peut être fait de deux manières:
- en modélisant l'occurrence d'un mot hors vocabulaire, considéré comme un token supplémentaire ajouté au vocabulaire prédéfini. Si un mot nouveau est rencontré dans le corpus de test, on le remplace par ce token unique (OOV). NB. cela assigne souvent une probabililité trop forte aux mots hors vocabulaires, puisque si chacun est individuellement rare, ils sont collectivement fréquents
- en utilisant des modèles infra-lexicaux, à base de caractères ou d'autres unités sous-lexicales. Les distributions conditionnelles utilisées pour la prédiction placent des paris sur tous les mots possibles.
- fait encore autre chose: puisqu'il ne fait pas les prédictions mot à mot, mais ignore même les découpages en mot.&lt;/p&gt;
&lt;p&gt;Il est raisonnable de comparer deux modèles qui choissent l'option (1), à supposer qu'ils aient la même taille de vocabulaire; et sans doute aussi deux modèles qui choisissent l'option (2), mais par contre il n'est pas raisonable de comparer deux modèles qui ne probabilisent pas le même espace.&lt;/p&gt;
&lt;h2&gt;Aux origines de la perplexité&lt;/h2&gt;
&lt;p&gt;L'origine de la perplexité provient de résultats de la théorie de l'information. Un premier concept utile est celui du taux d'entropie d'une suite de VA &lt;span class="math"&gt;\(X_{1} \dots X_n\)&lt;/span&gt; définie par [@Cover91information,chap4]:
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(P) = - \frac{1}{n} \sum_{x_1 \dots x_n} P(x_1 \dots x_n) \log P(x_1 \dots x_n)
$$&lt;/div&gt;
&lt;p&gt;
L'entropie est alors la limite pour &lt;span class="math"&gt;\(n\rightarrow{}\infty\)&lt;/span&gt; de &lt;span class="math"&gt;\( H(P) $. Sous certaines conditions (stationarité et ergodicité du processus $P\)&lt;/span&gt; qui génère les messages), on peut montrer que (a) cette limite existe ; (b), qu'elle est aussi égale à la limite de &lt;span class="math"&gt;\(\frac{1}{n} H(X_n|X_1 \dots X_{n-1})\)&lt;/span&gt; et que l'on peut calculer &lt;span class="math"&gt;\(H(P)\)&lt;/span&gt; en mesurant &lt;span class="math"&gt;\(\lim_{n \rightarrow \infty} \frac{1}{n} \log P(x_1 \x_n)\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;L'idée de perplexité provient de la mesure de la divergence KL entre la distribution "naturelle" des textes anglais et la distribution approchée par un modèle &lt;span class="math"&gt;\(M\)&lt;/span&gt;. Comme précédemment, la généralisation à des messages des de la divergence conduit à définir:
&lt;/p&gt;
&lt;div class="math"&gt;$$
KL(P||P_M) = lim_{n \rightarrow \infty}  \sum_{x_1 \dots x_n} p(x_1 \dots x_n) \\frac{log P(x_1 \dots x_n)}{P_M(x_1 \dots x_n)}
$$&lt;/div&gt;
&lt;p&gt; 
qui fait apparaitre deux termes: le premier est l'entropie, le second &lt;em&gt;l'entropie croisée&lt;/em&gt;. Comme pour la perplexité, ce terme est calculé comme &lt;span class="math"&gt;\(\lim_{n \rightarrow \infty} \frac{1}{n} \log P_M(x_1 \x_n)\)&lt;/span&gt; quand &lt;span class="math"&gt;\((x_1 \dots x_n)\)&lt;/span&gt; est une séquence tirée sous &lt;span class="math"&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;Une mise au point éclairante (en anglais)&lt;/h1&gt;
&lt;p&gt;[http://sjmielke.com/comparing-perplexities.htm]
(même si je ne suis pas complètement d'accord sur l'utilisation de LMs fermés, puisque dans la pratique on utilise un token unk).&lt;/p&gt;
&lt;h1&gt;Une comparaison cross-lingue qui contourne cet obstacle&lt;/h1&gt;
&lt;p&gt;@inproceedings{cotterell-etal-2018-languages,
    Abstract = {For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.},
    Address = {New Orleans, Louisiana},
    Author = {Cotterell, Ryan and Mielke, Sebastian J. and Eisner, Jason and Roark, Brian},
    Booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    Date-Added = {2019-04-25 12:31:09 +0200},
    Date-Modified = {2019-04-25 12:31:09 +0200},
    Doi = {10.18653/v1/N18-2085},
    Month = jun,
    Pages = {536--541},
    Publisher = {Association for Computational Linguistics},
    Title = {Are All Languages Equally Hard to Language-Model?},
    Url = {https://www.aclweb.org/anthology/N18-2085},
    Year = {2018},
    Bdsk-Url-1 = {https://www.aclweb.org/anthology/N18-2085},
    Bdsk-Url-2 = {https://doi.org/10.18653/v1/N18-2085}}&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;&lt;code&gt;
@articlet{shannon51prediction,
author={Claude Shannon},
title={Prediction and entropy of printed English},
journal={Bell System Technical Journal},
volume={30},
pages={50--64},
year={1951}
}&lt;/p&gt;
&lt;p&gt;@incollection{Gong18Frage,
title = {{FRAGE}: Frequency-Agnostic Word Representation},
author = {Gong, Chengyue and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {1334--1345},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7408-frage-frequency-agnostic-word-representation.pdf}
}&lt;/p&gt;
&lt;p&gt;@inproceedings{Hill16goldilocks,
  booktitle={International Conference on Learning Representations},
  year={2016},
  address = {San Juan, Puerto Rico},
  title={The Goldilocks principle: reading children’s books with explicit memory representations},
  author={Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason}
}&lt;/p&gt;
&lt;p&gt;@inproceedings{Yang2018breaking,
title={Breaking the Softmax Bottleneck: A High-Rank {RNN} Language Model},
author={Zhilin Yang and Zihang Dai and Ruslan Salakhutdinov and William W. Cohen},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HkwZSG-CZ},
}&lt;/p&gt;
&lt;p&gt;@inproceedings{Sutskever2011generating, 
  author = {Sutskever, Ilya and Martens, James and Hinton, Geoffrey E.},
  title = {Generating text with recurrent neural networks},
  booktitle = {Proceedings  of  the  28th  International  Conference on Machine Learning (ICML-11)},
  pages = {1017–1024}, 
  year = {2011}
}
@article{Shannon48amathematical,
  author = {Shannon, Claude E},
  title = {A Mathematical Theory of Communication},
  journal = {Bell System Technical Journal},
  volume = 27,
  number = 3,
  pages = {379-–423},
  doi = {doi:10.1002/j.1538-7305.1948.tb01338.x}
}&lt;/p&gt;
&lt;p&gt;&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a href="https://github.com/kpu/kenlm/issues/67"&gt;From KenLM discussion forum&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;If you want to compare perplexities across different vocabularies, pick a big number (larger than the union of all vocabularies you are considering), and use
&lt;code&gt;--vocab_pad BIG_NUMBER&lt;/code&gt;
with &lt;code&gt;lmplz&lt;/code&gt; when you train the model. This will train a model which is normalized with respect to the vocabulary size you specify.&lt;/p&gt;
&lt;p&gt;The default normalizes with respect to whatever vocabulary exists, plus 1 for the unknown word.&lt;/p&gt;
&lt;p&gt;Remember to do the same with whatever you compare to: if you train a neural network on a 200,000-word vocabulary, then its vocabulary is normalized with respect to 200,000 words including unknown. You should therefore take its unknown word probability times 1/(BIG_NUMBER-200,000) before computing perplexity. Effectively, the unknown probability is the probability of the class, not a specific unknown. So you need to hypothesize a uniform distribution. This is what &lt;code&gt;--vocab_pad&lt;/code&gt; does.&lt;/p&gt;
&lt;p&gt;Remember, the model with no training data has OOV probability 1 and therefore perplexity 0. Comparing perplexities with different vocabulary sizes is meaningless.&lt;/p&gt;
&lt;p&gt;You should really use including OOV, which is counts every word including unknown words. Excluding OOV skips unknown words for purposes of computing probability and normalizing. It's there because SRI does that.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="NLP, Language Models, TAL, Modèles de Langue"></category></entry></feed>